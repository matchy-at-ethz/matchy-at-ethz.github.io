<!DOCTYPE HTML>
<html lang="en" class="rust" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Notes @ ETH Zürich</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/custom.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "ayu" : "rust";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('rust')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item affix "><a href="index.html">Home</a></li><li class="spacer"></li><li class="chapter-item affix "><li class="part-title">22HS</li><li class="chapter-item "><div><strong aria-hidden="true">1.</strong> Computational Biology</div></li><li class="chapter-item "><a href="22hs/dm1/data_mining_i.html"><strong aria-hidden="true">2.</strong> Data Mining I</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="22hs/dm1/01_metrics.html"><strong aria-hidden="true">2.1.</strong> Metrics</a></li><li class="chapter-item "><a href="22hs/dm1/02_classification.html"><strong aria-hidden="true">2.2.</strong> Classification</a></li><li class="chapter-item "><a href="22hs/dm1/03_clustering.html"><strong aria-hidden="true">2.3.</strong> Clustering</a></li><li class="chapter-item "><a href="22hs/dm1/04_feature_selection.html"><strong aria-hidden="true">2.4.</strong> Feature Selection</a></li><li class="chapter-item "><a href="22hs/dm1/05_applications_in_computational_biology.html"><strong aria-hidden="true">2.5.</strong> Applications in Computational Biology</a></li><li class="spacer"></li></ol></li><li class="chapter-item "><li class="part-title">23FS + HS</li><li class="chapter-item "><a href="23fs/mham/mobile_health_and_activity_monitoring.html"><strong aria-hidden="true">3.</strong> Mobile Health and Activity Monitoring</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><div><strong aria-hidden="true">3.1.</strong> Introduction</div></li></ol></li><li class="chapter-item "><a href="23fs/bdfe/big_data_for_engineers.html"><strong aria-hidden="true">4.</strong> Big Data for Engineers</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="23fs/bdfe/01_azure_blob_storage.html"><strong aria-hidden="true">4.1.</strong> Azure Blob Storage</a></li><li class="chapter-item "><a href="23fs/bdfe/02_hadoop.html"><strong aria-hidden="true">4.2.</strong> Hadoop</a></li><li class="chapter-item "><a href="23fs/bdfe/03_spark.html"><strong aria-hidden="true">4.3.</strong> Spark</a></li><li class="chapter-item "><a href="23fs/bdfe/04_jsoniq.html"><strong aria-hidden="true">4.4.</strong> JSONiq</a></li><li class="chapter-item "><a href="23fs/bdfe/05_hbase_(wide_column_store).html"><strong aria-hidden="true">4.5.</strong> HBase (Wide Column Store)</a></li><li class="chapter-item "><a href="23fs/bdfe/06_mapreduce.html"><strong aria-hidden="true">4.6.</strong> MapReduce</a></li><li class="chapter-item "><a href="23fs/bdfe/07_yarn.html"><strong aria-hidden="true">4.7.</strong> YARN</a></li></ol></li><li class="chapter-item "><a href="23fs/fg/functional_genomics.html"><strong aria-hidden="true">5.</strong> Functional Genomics</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="23fs/fg/01_modern_genomics_i.html"><strong aria-hidden="true">5.1.</strong> Modern Genomics I</a></li><li class="chapter-item "><a href="23fs/fg/02_modern_genomics_ii.html"><strong aria-hidden="true">5.2.</strong> Modern Genomics II</a></li><li class="chapter-item "><a href="23fs/fg/03_transcriptomics_i.html"><strong aria-hidden="true">5.3.</strong> Transcriptomics I</a></li><li class="chapter-item "><a href="23fs/fg/04_transcriptomics_ii.html"><strong aria-hidden="true">5.4.</strong> Transcriptomics II</a></li><li class="chapter-item "><div><strong aria-hidden="true">5.5.</strong> MicroRNAs and other small RNAs</div></li><li class="chapter-item "><div><strong aria-hidden="true">5.6.</strong> Proteomics</div></li><li class="chapter-item "><div><strong aria-hidden="true">5.7.</strong> Metabolomics</div></li><li class="chapter-item "><div><strong aria-hidden="true">5.8.</strong> Single Cell Mass Cytometry</div></li><li class="chapter-item "><div><strong aria-hidden="true">5.9.</strong> Protein Networks</div></li><li class="chapter-item "><div><strong aria-hidden="true">5.10.</strong> Epigenomics and Gene Regulation</div></li><li class="chapter-item "><div><strong aria-hidden="true">5.11.</strong> Qualitiy Control and Standards</div></li><li class="spacer"></li></ol></li><li class="chapter-item "><li class="part-title">24FS</li><li class="chapter-item "><div><strong aria-hidden="true">6.</strong> Biofluiddynamics</div></li><li class="chapter-item "><div><strong aria-hidden="true">7.</strong> Synthetic Biology I</div></li><li class="chapter-item affix "><li class="part-title">24HS</li><li class="chapter-item "><div><strong aria-hidden="true">8.</strong> Computer Architecture</div></li><li class="chapter-item "><div><strong aria-hidden="true">9.</strong> Applied Bioinformatics: Microbiomes</div></li><li class="chapter-item "><a href="24hs/evodynamo/evolutionary_dynamics.html"><strong aria-hidden="true">10.</strong> Evolutionary Dynamics</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="24hs/evodynamo/01_what_is_evolution.html"><strong aria-hidden="true">10.1.</strong> What is Evolution?</a></li><li class="chapter-item "><a href="24hs/evodynamo/02_quasispecies.html"><strong aria-hidden="true">10.2.</strong> Quasispecies</a></li><li class="chapter-item "><a href="24hs/evodynamo/03_stochastic_modles_of_finite_populations.html"><strong aria-hidden="true">10.3.</strong> Stochastic Models of Finite Populations</a></li><li class="chapter-item "><a href="24hs/evodynamo/04_evolutionary_dynamics_of_cancer.html"><strong aria-hidden="true">10.4.</strong> Evolutionary Dynamics of Cancer</a></li><li class="chapter-item "><div><strong aria-hidden="true">10.5.</strong> Cancer Progression: The Speed of Adaptation</div></li><li class="chapter-item "><div><strong aria-hidden="true">10.6.</strong> Diffusion Theory</div></li><li class="chapter-item "><div><strong aria-hidden="true">10.7.</strong> Evolutionary Game Theory</div></li><li class="chapter-item "><div><strong aria-hidden="true">10.8.</strong> Evolutionary Games in Finite Populations</div></li><li class="chapter-item "><div><strong aria-hidden="true">10.9.</strong> Spatial Models for the Evolution of Solid Tumors</div></li><li class="chapter-item "><div><strong aria-hidden="true">10.10.</strong> Branching Processes</div></li><li class="chapter-item "><div><strong aria-hidden="true">10.11.</strong> Evolutionary Escape</div></li><li class="chapter-item "><div><strong aria-hidden="true">10.12.</strong> Coalescent Theory</div></li><li class="chapter-item "><div><strong aria-hidden="true">10.13.</strong> Tumor Archeology</div></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Notes @ ETH Zürich</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="notes-taken--eth-zurich"><a class="header" href="#notes-taken--eth-zurich">Notes taken @ ETH Zurich</a></h1>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>Here I list all courses I took at ETH Zurich and provide a brief overview of the course content. The clickable links will lead you to the respective course notes.</p>
<p>Courses are organized by semester and year. The course notes are organized by lecture.</p>
<h3 id="22hs"><a class="header" href="#22hs">22HS</a></h3>
<h4 id="eth-zürich"><a class="header" href="#eth-zürich">@ETH Zürich</a></h4>
<ul>
<li>Computational Biology</li>
<li><a href="./22hs/dm1/data_mining_i.html">Data Mining I</a></li>
<li>Information Systems for Engineers (Won't be covered here)</li>
<li>Computational Systems Biology</li>
<li>Concepts of OBject-Oriented Programming</li>
</ul>
<h4 id="universität-basel"><a class="header" href="#universität-basel">@Universität Basel</a></h4>
<ul>
<li>Current Topics in Biophysics</li>
<li>Molecular Medicine I</li>
<li>Molecular Control of Vertebrate Development and Organogenesis</li>
</ul>
<h3 id="23fs--hs"><a class="header" href="#23fs--hs">23FS + HS</a></h3>
<h4 id="eth-zürich-1"><a class="header" href="#eth-zürich-1">@ETH Zürich</a></h4>
<ul>
<li><a href="./23fs/mham/mobile_health_and_activity_monitoring.html">Mobile Health and Activity Monitoring</a></li>
<li>Introduction to Machine Learning</li>
<li><a href="./23fs/bdfe/big_data_for_engineers.html">Big Data for Engineers</a></li>
<li>Statistical Models in Computational Biology</li>
</ul>
<h4 id="universität-zürich"><a class="header" href="#universität-zürich">@Universität Zürich</a></h4>
<ul>
<li><a href="./23fs/fg/functional_genomics.html">Functional Genomics</a></li>
</ul>
<h4 id="universität-basel-1"><a class="header" href="#universität-basel-1">@Universität Basel</a></h4>
<ul>
<li>High Performance Computing</li>
<li>Distributed Systems (Dropped, but auditted till the end)</li>
<li>Molecular Medicine II</li>
<li>Chromatin and Epigenetics</li>
</ul>
<p>No courses (except for seminar) taken in 23HS since I was doing an internship at Roche.</p>
<h3 id="24fs"><a class="header" href="#24fs">24FS</a></h3>
<ul>
<li>Biofluiddynamics</li>
<li>Synthetic Biology I</li>
</ul>
<h3 id="24hs"><a class="header" href="#24hs">24HS</a></h3>
<ul>
<li>Computer Architecture</li>
<li>Applied Bioinformatics: Microbiomes</li>
<li>Algorithms and Data Structures for Population Scale Genomics (Audit; <em>Probably won't be covered here</em>)</li>
<li>Probabilistic Artificial Intelligence (Audit; <em>Probably won't be covered here</em>)</li>
<li><a href="./24hs/evodynamo/evolutionary_dynamics.html">Evolutionary Dynamics</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-mining-i"><a class="header" href="#data-mining-i">Data Mining I</a></h1>
<p>A course taught by <a href="https://www.biochem.mpg.de/borgwardt">Prof. Karsten Borgwardt</a></p>
<p>There used to be Data Minining I and II, but since 2022 fall was the last time this course was offered, I only took Data Mining I.</p>
<p>The course was <em>really</em> well organized and the concepts were explained <em>really</em> clear, but it's <em>not</em> for beginners (students biological background) without solid mathematical foundation. My classmates with a biology background struggled a lot.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="22hs/dm1/01_metrics.html">01. Metrics</a></li>
<li><a href="22hs/dm1/02_classification.html">02. Classification</a></li>
<li><a href="22hs/dm1/03_clustering.html">03. Clustering</a></li>
<li><a href="22hs/dm1/04_feature_selection.html">04. Feature Selection</a></li>
<li><a href="22hs/dm1/05_applications_in_computational_biology.html">05. Applications in Computational Biology</a></li>
</ul>
<!-- toc -->
<div style="break-before: page; page-break-before: always;"></div><h1 id="metrics"><a class="header" href="#metrics">Metrics</a></h1>
<h2 id="introduction-to-data-mining-dm"><a class="header" href="#introduction-to-data-mining-dm"><strong>Introduction to Data Mining (DM)</strong></a></h2>
<ul>
<li><strong>Definition</strong>: Data mining is the search for patterns and statistical dependencies in large datasets. It is often associated with machine learning or knowledge discovery.</li>
<li><strong>Application</strong>: DM is applied in biology, medicine, and various industries for personalized recommendations, personalized medicine, etc.</li>
<li><strong>Key Topics</strong>: Distance functions, classification, clustering, feature selection.</li>
</ul>
<hr />
<h2 id="personalized-medicine-through-data-mining"><a class="header" href="#personalized-medicine-through-data-mining"><strong>Personalized Medicine through Data Mining</strong></a></h2>
<ul>
<li><strong>Vision</strong>: Tailor treatments to patients’ genetic/molecular properties to increase efficacy. Drugs often work for only a fraction of patients due to genetic variability.</li>
<li><strong>Technological Advances</strong>: Genome sequencing has become rapid and scalable, identifying variations among millions of genetic bases.</li>
<li><strong>Goals</strong>: Detect correlations between diseases, drug responses, and genetic variations.</li>
</ul>
<hr />
<h2 id="distance-functions"><a class="header" href="#distance-functions"><strong>Distance Functions</strong></a></h2>
<ul>
<li><strong>Key Concept</strong>: Similarity or distance between objects is at the core of data mining.</li>
<li><strong>Metric Definition</strong>: A metric on vectors \(x_1, x_2, x_3 \in \mathbb{R}^d\) is a function \(d\) if:
<ol>
<li>\( d(x_1, x_2) \geq 0 \)</li>
<li>\( d(x_1, x_2) = 0 \text{ if and only if } x_1 = x_2 \)</li>
<li>\( d(x_1, x_2) = d(x_2, x_1) \)</li>
<li>\( d(x_1, x_3) \leq d(x_1, x_2) + d(x_2, x_3) \)</li>
</ol>
</li>
</ul>
<hr />
<h2 id="common-distance-functions"><a class="header" href="#common-distance-functions"><strong>Common Distance Functions</strong></a></h2>
<ul>
<li><strong>Manhattan Distance</strong>: \( d(x, x') = \sum_{i=1}^d |x_i - x'_i| \)</li>
<li><strong>Euclidean Distance</strong>: \( d(x, x') = \sqrt{\sum_{i=1}^d (x_i - x'_i)^2} \)</li>
<li><strong>Chebyshev Distance</strong>: \( d(x, x') = \max_i (|x_i - x'_i|) \)</li>
<li><strong>Minkowski Distance</strong>: \( d(x, x') = \left(\sum_{i=1}^d |x_i - x'_i|^p \right)^{1/p} \)
<ul>
<li>\( p = 1 \) recovers the Manhattan distance, \( p = 2 \) recovers the Euclidean distance, and for \( p \rightarrow \infty \), it converges to the Chebyshev distance.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="distance-on-sets-and-strings"><a class="header" href="#distance-on-sets-and-strings"><strong>Distance on Sets and Strings</strong></a></h2>
<ul>
<li><strong>Jaccard Distance</strong>: Measures dissimilarity between sets \( A \) and \( B \).
<ul>
<li>Formula: \( d(A, B) = \frac{|A \cup B| - |A \cap B|}{|A \cup B|} \)</li>
</ul>
</li>
<li><strong>String Similarity (k-mer)</strong>: Quantifies similarity by representing strings as histograms of k-mer frequencies. Example: the strings &quot;downtown&quot; and &quot;known&quot; can be compared based on their 3-mer substrings.</li>
</ul>
<hr />
<h2 id="distance-on-time-series"><a class="header" href="#distance-on-time-series"><strong>Distance on Time Series</strong></a></h2>
<ul>
<li><strong>Dynamic Time Warping (DTW)</strong>: Measures similarity between time series of different lengths or varying time intervals.
<ul>
<li>Recursive formula:
[
DTW(i, j) = d(x_i, x'_j) + \min\left{ DTW(i, j-1), DTW(i-1, j), DTW(i-1, j-1) \right}
]</li>
<li>Used when direct time-to-time comparisons are not feasible.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="distance-on-graphs"><a class="header" href="#distance-on-graphs"><strong>Distance on Graphs</strong></a></h2>
<ul>
<li><strong>Graph Comparison Problems</strong>: Key problems include determining if two graphs are identical (graph isomorphism) or finding if one graph is contained within another (subgraph isomorphism).</li>
<li><strong>Weisfeiler-Lehman Kernel</strong>: Efficient method for graph comparison by iterating over node neighborhoods, compressing them, and relabeling based on sorted labels.
<ul>
<li>This method is highly scalable and is commonly used in chemoinformatics and bioinformatics.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="applications-in-biology--medicine"><a class="header" href="#applications-in-biology--medicine"><strong>Applications in Biology &amp; Medicine</strong></a></h2>
<ul>
<li><strong>Data Mining in Genetics</strong>: Searches for disease-associated loci in genomes. Challenges include:
<ul>
<li><strong>Missing Heritability</strong>: Many diseases show weak correlations with genetic loci due to small sample sizes, environmental factors, and oversimplified models.</li>
<li><strong>Interaction Search</strong>: Efficient algorithms are needed for exploring interactions between millions of genetic loci without exhaustive enumeration.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="key-concepts-in-dm"><a class="header" href="#key-concepts-in-dm"><strong>Key Concepts in DM</strong></a></h2>
<ul>
<li><strong>Knowledge Discovery Process</strong>: Stages include data cleaning, integration, selection, transformation, mining, pattern evaluation, and presentation.</li>
<li><strong>Outlier Detection</strong>: Finding patients with unusual disease progression or drug response is a key challenge in personalized medicine.</li>
</ul>
<hr />
<h2 id="algorithms-and-future-of-data-mining"><a class="header" href="#algorithms-and-future-of-data-mining"><strong>Algorithms and Future of Data Mining</strong></a></h2>
<ul>
<li><strong>Graph Kernels</strong>: Algorithms for comparing large graphs (like biological networks) are critical in drug discovery and gene interaction studies.</li>
<li><strong>Future Trends</strong>: Increasing use of wearable devices, electronic health records, and indirect monitoring through social media is expected to fuel personalized medicine.</li>
</ul>
<hr />
<h2 id="figures-and-diagrams-explanation"><a class="header" href="#figures-and-diagrams-explanation">Figures and Diagrams (Explanation)</a></h2>
<ul>
<li><strong>Dynamic Time Warping (DTW)</strong>: A matrix-based visual method for aligning two time series optimally by minimizing alignment costs.</li>
<li><strong>Weisfeiler-Lehman (WL) Kernel</strong>: Shows the iterative process of graph comparison by relabeling nodes based on their neighbor labels.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="classification-algorithms"><a class="header" href="#classification-algorithms">Classification Algorithms</a></h1>
<h2 id="classification-overview"><a class="header" href="#classification-overview"><strong>Classification Overview</strong></a></h2>
<ul>
<li><strong>Problem</strong>: Given an object \(x\), predict its class label \(y\). Examples include identifying objects in computer vision, detecting fraudulent credit card transactions, and gene classification in personalized medicine.</li>
<li><strong>Types</strong>:
<ul>
<li><strong>Binary classification</strong>: \(y \in {0, 1}\)</li>
<li><strong>Multiclass classification</strong>: \(y \in {1, \dots, n}\)</li>
<li><strong>Regression</strong>: \(y \in \mathbb{R}\)</li>
</ul>
</li>
</ul>
<hr />
<h2 id="evaluating-classifiers"><a class="header" href="#evaluating-classifiers"><strong>Evaluating Classifiers</strong></a></h2>
<ul>
<li>
<p><strong>Contingency Table</strong>: For binary classification, results are represented as:</p>
<p>|       | \(y = 1\) |\( y = -1 \)|
|-------|-------|--------|
| \( f(x) = 1 \) | TP    | FP     |
|\( f(x) = -1 \)| FN    | TN     |</p>
<p>TP: True Positive, FP: False Positive, FN: False Negative, TN: True Negative.</p>
</li>
<li>
<p><strong>Accuracy</strong>:
\[
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\]
Accuracy is not ideal for imbalanced classes, which leads to focusing on <strong>precision</strong> and <strong>recall</strong>:</p>
</li>
<li>
<p><strong>Precision</strong>: \( \frac{TP}{TP + FP} \)</p>
</li>
<li>
<p><strong>Recall</strong>: \( \frac{TP}{TP + FN} \)</p>
</li>
<li>
<p><strong>ROC Curve</strong>: Plots <strong>true positive rate</strong> (sensitivity) vs. <strong>false positive rate</strong>. A perfect classifier's curve passes through the point (0,1). The <strong>Area Under the Curve (AUC)</strong> is a metric summarizing the performance of the classifier:
\[
AUC = P\left(\text{positive point gets higher score than negative}\right)
\]</p>
</li>
</ul>
<hr />
<h2 id="cross-validation-and-bootstrapping"><a class="header" href="#cross-validation-and-bootstrapping"><strong>Cross-Validation and Bootstrapping</strong></a></h2>
<ul>
<li><strong>Cross-Validation</strong>: Split data into \(k\) subsets, train on \(k-1\), and test on the remaining subset. This is repeated \(k\) times.</li>
<li><strong>Bootstrapping</strong>: Random sampling with replacement to create multiple training/test splits, averaging results over multiple iterations.</li>
<li><strong>Parameter Tuning</strong>: Use internal cross-validation on training data to optimize model parameters without overfitting.</li>
</ul>
<hr />
<h2 id="nearest-neighbors-k-nn"><a class="header" href="#nearest-neighbors-k-nn"><strong>Nearest Neighbors (k-NN)</strong></a></h2>
<ul>
<li>
<p><strong>k-NN</strong>: Classifies a point based on the majority label of its \(k\) nearest neighbors.</p>
<ul>
<li><strong>Distance Metric</strong>: The Euclidean distance is often used:
\[
d(x, x') = \sqrt{\sum_{i=1}^d (x_i - x'_i)^2}
\]</li>
<li><strong>Challenges</strong>: Selecting \(k\), runtime optimization, and handling high-dimensional data.</li>
</ul>
</li>
<li>
<p><strong>Mahalanobis Distance</strong>:
\[
d_M(x, x') = \sqrt{(x - x')^\top \Sigma^{-1} (x - x')}
\]
where \(\Sigma\) is the covariance matrix of the dataset.</p>
</li>
</ul>
<hr />
<h2 id="naive-bayes-classifier"><a class="header" href="#naive-bayes-classifier"><strong>Naive Bayes Classifier</strong></a></h2>
<ul>
<li>
<p><strong>Bayes' Rule</strong>:
\[
P(Y = y | X = x) = \frac{P(X = x | Y = y) P(Y = y)}{P(X = x)}
\]</p>
<ul>
<li>The classifier assumes <strong>conditional independence</strong> of features:
\[
P(X | Y = y) = \prod_{j=1}^d P(X_j | Y = y)
\]</li>
</ul>
</li>
<li>
<p><strong>Prediction</strong>:
\[
\hat{y} = \arg\max_{y} P(Y = y) \prod_{j=1}^d P(X_j | Y = y)
\]</p>
<ul>
<li>Naive Bayes works well in practice despite the strong independence assumption.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="linear-discriminant-analysis-lda"><a class="header" href="#linear-discriminant-analysis-lda"><strong>Linear Discriminant Analysis (LDA)</strong></a></h2>
<ul>
<li>
<p><strong>Assumptions</strong>:</p>
<ul>
<li>Data from each class is normally distributed with the same covariance matrix but different means \(\mu_0\) and \(\mu_1\).</li>
</ul>
</li>
<li>
<p><strong>Log-Likelihood Ratio</strong>:
\[
f(x) = \log\left( \frac{P(Y=1|X=x)}{P(Y=0|X=x)} \right)
\]</p>
<ul>
<li>Linear discriminant: \( f(x) = w^\top x + b \), where \(w = (\mu_1 - \mu_0)^\top \Sigma^{-1}\).</li>
</ul>
</li>
</ul>
<hr />
<h2 id="logistic-regression"><a class="header" href="#logistic-regression"><strong>Logistic Regression</strong></a></h2>
<ul>
<li>
<p><strong>Logistic Function</strong>:
\[
f(z) = \frac{1}{1 + e^{-z}}
\]
where \(z = w^\top x + b\).</p>
</li>
<li>
<p><strong>Training</strong>: Minimizing the logistic loss:
\[
\mathcal{L}(w) = \frac{1}{n} \sum_{i=1}^n \log(1 + e^{-y_i (w^\top x_i)})
\]</p>
<ul>
<li>The weights \(w\) are learned by gradient descent or other optimization techniques.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="decision-trees"><a class="header" href="#decision-trees"><strong>Decision Trees</strong></a></h2>
<ul>
<li><strong>Concept</strong>: Split the data recursively based on feature values to maximize information gain, using criteria like <strong>entropy</strong> and the <strong>Gini index</strong>:
<ul>
<li><strong>Entropy</strong>:
\[
H(D) = -\sum_{i=1}^m p_i \log_2(p_i)
\]</li>
<li><strong>Gini Index</strong>:
\[
Gini(D) = 1 - \sum_{i=1}^m p_i^2
\]</li>
</ul>
</li>
<li><strong>Random Forests</strong>: Ensemble of decision trees, each built on random subsets of data and features.</li>
</ul>
<hr />
<h2 id="support-vector-machines-svm"><a class="header" href="#support-vector-machines-svm"><strong>Support Vector Machines (SVM)</strong></a></h2>
<ul>
<li><strong>Hard-Margin SVM</strong>: Finds the hyperplane that maximizes the margin between classes:
\[
\min_w \frac{1}{2} |w|^2 \quad \text{s.t.} , y_i (w^\top x_i + b) \geq 1
\]</li>
<li><strong>Soft-Margin SVM</strong>: Allows some misclassification using slack variables \(\xi\):
\[
\min_w \frac{1}{2} |w|^2 + C \sum_{i=1}^n \xi_i \quad \text{s.t.} , y_i (w^\top x_i + b) \geq 1 - \xi_i
\]
where \(C\) controls the trade-off between margin size and misclassification.</li>
</ul>
<hr />
<h2 id="kernel-methods"><a class="header" href="#kernel-methods"><strong>Kernel Methods</strong></a></h2>
<ul>
<li><strong>Kernel Trick</strong>: Maps data into a higher-dimensional space to make it linearly separable. Common kernels include:
<ul>
<li><strong>Linear Kernel</strong>: \( k(x, x') = x^\top x' \)</li>
<li><strong>Polynomial Kernel</strong>: \( k(x, x') = (x^\top x' + c)^d \)</li>
<li><strong>Gaussian (RBF) Kernel</strong>:
\[
k(x, x') = \exp\left(- \frac{|x - x'|^2}{2\sigma^2}\right)
\]</li>
</ul>
</li>
</ul>
<hr />
<h2 id="figuresdiagrams"><a class="header" href="#figuresdiagrams"><strong>Figures/Diagrams</strong></a></h2>
<ol>
<li><strong>ROC Curves</strong>: Visualization of model performance with true positive vs. false positive rates.</li>
<li><strong>Logistic Function</strong>: S-shaped curve representing the probability output from logistic regression.</li>
<li><strong>Decision Trees</strong>: Flowchart-like structures splitting data based on feature values.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="clustering"><a class="header" href="#clustering">Clustering</a></h1>
<h2 id="what-is-clustering"><a class="header" href="#what-is-clustering"><strong>What is Clustering?</strong></a></h2>
<ul>
<li><strong>Definition</strong>: Clustering is the task of grouping a set of objects such that objects in the same group (cluster) are more similar to each other than to those in other groups. It is an example of <strong>unsupervised learning</strong> since no predefined labels (training data) are used.</li>
<li><strong>Applications</strong>:
<ul>
<li>Grouping images to discover categories.</li>
<li>Clustering patient data to uncover disease subtypes.</li>
<li>Detecting communities in social networks.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="k-means-clustering"><a class="header" href="#k-means-clustering"><strong>k-means Clustering</strong></a></h2>
<ul>
<li>
<p><strong>Objective</strong>: Partition data into \( k \) clusters by minimizing the variance within each cluster:
\[
V(D) = \sum_{i=1}^{k} \sum_{x_j \in S_i} (x_j - \mu_i)^2
\]</p>
<ul>
<li>\( S_i \): the \(i\)-th cluster</li>
<li>\( \mu_i \): the mean of the cluster</li>
<li>\( D \): dataset</li>
</ul>
</li>
<li>
<p><strong>Algorithm (Lloyd's Algorithm)</strong>:</p>
<ol>
<li>Partition data into \( k \) initial clusters.</li>
<li>Compute the mean for each cluster.</li>
<li>Reassign each point to the closest cluster mean.</li>
<li>Repeat until no point changes its cluster.</li>
</ol>
</li>
<li>
<p><strong>Challenges</strong>:</p>
<ul>
<li>Sensitive to the choice of \( k \) (number of clusters).</li>
<li>Initialization affects the final result, and the algorithm might converge to a <strong>local optimum</strong>.</li>
<li><strong>Order-dependent</strong>: the final clusters depend on the initial configuration.</li>
</ul>
</li>
<li>
<p><strong>Silhouette Coefficient</strong>: A metric to evaluate the quality of clustering:
\[
s(x) = \frac{d(x, \mu_{C'}) - d(x, \mu_C)}{\max(d(x, \mu_C), d(x, \mu_{C'})}
\]</p>
<ul>
<li>\( s(x) \approx 1 \): Well clustered.</li>
<li>\( s(x) \approx 0 \): Between clusters.</li>
<li>\( s(x) &lt; 0 \): Incorrectly clustered.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="k-medoids-clustering"><a class="header" href="#k-medoids-clustering"><strong>k-medoids Clustering</strong></a></h2>
<ul>
<li><strong>Similar to k-means</strong>, but instead of the mean, it uses the <strong>medoid</strong> (the most centrally located point in a cluster).
<ul>
<li>Formula for the medoid:
\[
m_i = \arg \min_{x_j \in S_i} ||x_j - \mu_i||^2
\]</li>
<li>This method is more <strong>robust</strong> to outliers since it restricts cluster centers to actual data points.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="kernel-k-means"><a class="header" href="#kernel-k-means"><strong>Kernel k-means</strong></a></h2>
<ul>
<li><strong>Idea</strong>: Apply k-means clustering in a high-dimensional feature space using <strong>kernels</strong> to handle complex, non-linear boundaries.
<ul>
<li>Key step: Instead of directly computing distances, use kernel functions:
\[
d(x_1, x_2) = | \phi(x_1) - \phi(x_2) |^2 = k(x_1, x_1) - 2k(x_1, x_2) + k(x_2, x_2)
\]</li>
<li>Kernel k-means is especially useful for clustering <strong>graphs</strong> or <strong>strings</strong>.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="graph-based-clustering"><a class="header" href="#graph-based-clustering"><strong>Graph-Based Clustering</strong></a></h2>
<ul>
<li>
<p><strong>Graph Representation</strong>: A dataset is represented as a graph \( G = (V, E) \), where nodes \( V \) are objects and edges \( E \) are weighted by the similarity between objects.</p>
</li>
<li>
<p><strong>Steps</strong>:</p>
<ol>
<li>Define a threshold \( \theta \).</li>
<li>Remove all edges with weight \( w_{ij} &gt; \theta \).</li>
<li>Each <strong>connected component</strong> in the graph forms a cluster.</li>
</ol>
</li>
<li>
<p><strong>DBScan (Density-Based Spatial Clustering of Applications with Noise)</strong>:</p>
<ul>
<li><strong>Core Idea</strong>: Group points that are closely packed together and mark outliers as noise.</li>
<li><strong>Core Object</strong>: A point is considered a core object if there are at least <strong>MinPoints</strong> neighbors within a distance \( \epsilon \). Clusters are built by iteratively expanding core objects.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="spectral-clustering"><a class="header" href="#spectral-clustering"><strong>Spectral Clustering</strong></a></h2>
<ul>
<li>
<p><strong>Concept</strong>: Uses the <strong>graph Laplacian</strong> to connect graph-based clustering with k-means.</p>
<ul>
<li>The <strong>Laplacian matrix</strong> \( L = D - W \), where \( D \) is the degree matrix and \( W \) is the adjacency matrix, helps find clusters by minimizing a <strong>cut</strong> in the graph:
\[
\min \frac{1}{2} \sum_{a=1}^{k} \sum_{b=1}^{k} \kappa(C_a, C_b)
\]
where \( \kappa \) measures the weight of edges between clusters.</li>
</ul>
</li>
<li>
<p><strong>Procedure</strong>:</p>
<ol>
<li>Compute the <strong>eigenvectors</strong> of the Laplacian matrix.</li>
<li>Use the <strong>k smallest eigenvectors</strong> to form a new representation of the data.</li>
<li>Apply k-means to this new representation.</li>
</ol>
</li>
</ul>
<hr />
<h2 id="em-clustering-expectation-maximization"><a class="header" href="#em-clustering-expectation-maximization"><strong>EM Clustering (Expectation-Maximization)</strong></a></h2>
<ul>
<li>
<p><strong>Soft k-means</strong>: Instead of hard assignments, points are assigned probabilistically to clusters.</p>
<ul>
<li><strong>E-step</strong>: Compute the probability that each point belongs to each cluster.</li>
<li><strong>M-step</strong>: Update the cluster parameters (means and covariances) based on these probabilities.</li>
</ul>
</li>
<li>
<p><strong>Gaussian Mixture Models (GMMs)</strong>: EM is often used to estimate the parameters for mixtures of Gaussian distributions.</p>
</li>
</ul>
<hr />
<h2 id="hierarchical-clustering"><a class="header" href="#hierarchical-clustering"><strong>Hierarchical Clustering</strong></a></h2>
<ul>
<li>
<p><strong>Concept</strong>: Instead of flat clusters, it builds a <strong>hierarchy</strong> of clusters where clusters can contain subclusters.</p>
</li>
<li>
<p><strong>Methods</strong>:</p>
<ul>
<li><strong>Single Link</strong>: Minimum distance between points in two clusters.</li>
<li><strong>Complete Link</strong>: Maximum distance between points in two clusters.</li>
<li><strong>Average Link</strong>: Average distance between all pairs of points in two clusters.</li>
</ul>
</li>
<li>
<p><strong>Dendrogram</strong>: A tree-like structure used to represent the hierarchy.</p>
</li>
</ul>
<hr />
<h2 id="comparison-of-clustering-algorithms"><a class="header" href="#comparison-of-clustering-algorithms"><strong>Comparison of Clustering Algorithms</strong></a></h2>
<ul>
<li>
<p><strong>k-means</strong>:</p>
<ul>
<li>Fast and simple.</li>
<li>Sensitive to initialization.</li>
<li>Good for large datasets with clear cluster boundaries.</li>
</ul>
</li>
<li>
<p><strong>k-medoids</strong>: More robust to outliers but slower than k-means.</p>
</li>
<li>
<p><strong>Graph-based Clustering (DBScan)</strong>:</p>
<ul>
<li>Handles noise well.</li>
<li>No need to specify the number of clusters.</li>
<li>Struggles with varying density clusters.</li>
</ul>
</li>
<li>
<p><strong>Spectral Clustering</strong>: Good for complex data but computationally expensive.</p>
</li>
<li>
<p><strong>EM Clustering</strong>: Handles soft assignments and works well with <strong>Gaussian mixtures</strong>, but prone to convergence to local optima.</p>
</li>
<li>
<p><strong>Hierarchical Clustering</strong>: Captures the nested structure but can be computationally expensive for large datasets.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="feature-selection"><a class="header" href="#feature-selection">Feature Selection</a></h1>
<h2 id="what-is-feature-selection"><a class="header" href="#what-is-feature-selection"><strong>What is Feature Selection?</strong></a></h2>
<ul>
<li><strong>Definition</strong>: Feature selection involves identifying a relevant subset of features \( X \) that are most predictive of the output variable \( Y \) in supervised learning tasks.</li>
<li><strong>Distinction</strong>:
<ul>
<li><strong>Feature Ranking</strong>: Orders features by relevance.</li>
<li><strong>Feature Transformation</strong>: Transforms original features into a new representation.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="why-feature-selection"><a class="header" href="#why-feature-selection"><strong>Why Feature Selection?</strong></a></h2>
<ul>
<li><strong>Goals</strong>:
<ul>
<li>Detect causal relationships.</li>
<li>Remove noisy or irrelevant features.</li>
<li>Reduce computational cost and improve interpretability.</li>
<li>Speed up learning and improve accuracy.</li>
</ul>
</li>
<li><strong>Two Modes</strong>:
<ul>
<li><strong>Filter Approach</strong>: Select features a priori based on a quality metric (e.g., information criterion).</li>
<li><strong>Wrapper Approach</strong>: Select features specific to a learning algorithm.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="feature-selection-as-an-optimization-problem"><a class="header" href="#feature-selection-as-an-optimization-problem"><strong>Feature Selection as an Optimization Problem</strong></a></h2>
<ul>
<li><strong>Objective</strong>: Given a feature set \( D \) and a quality function \( q \), select the subset \( S \) of size \( n' \) that maximizes \( q \):
\[
\arg \max_{S \subset D \land |S|=n'} q(S)
\]</li>
<li><strong>Challenges</strong>: The problem is combinatorial and computationally intractable (exponential in \( n' \)).</li>
</ul>
<hr />
<h2 id="greedy-feature-selection"><a class="header" href="#greedy-feature-selection"><strong>Greedy Feature Selection</strong></a></h2>
<ul>
<li>
<p><strong>Forward Feature Selection</strong>:</p>
<ul>
<li>Start with an empty set.</li>
<li>Iteratively add the feature that maximizes the quality function until the desired number of features is selected.
\[
S^{\ast} \leftarrow S^{\ast} \cup \arg \max_j q(S^{\ast} \cup j)
\]</li>
</ul>
</li>
<li>
<p><strong>Backward Elimination</strong>:</p>
<ul>
<li>Start with all features.</li>
<li>Iteratively remove the least informative feature.
\[
S^{\ast} \leftarrow S^{\ast} \setminus \arg \max_j q(S^{\ast} \setminus j)
\]</li>
</ul>
</li>
<li>
<p><strong>Optimality</strong>: Greedy approaches are optimal if the quality function \( q \) is <strong>additive</strong> or <strong>submodular</strong> (exhibits diminishing returns).</p>
</li>
</ul>
<hr />
<h2 id="key-metrics-for-feature-selection"><a class="header" href="#key-metrics-for-feature-selection"><strong>Key Metrics for Feature Selection</strong></a></h2>
<ol>
<li>
<p><strong>Correlation Coefficient</strong> \( \rho_{X, Y} \):
\[
\rho_{X, Y} = \frac{\text{cov}(X, Y)}{\sigma_X \sigma_Y}
\]
Measures linear dependence between features \( X \) and \( Y \).</p>
</li>
<li>
<p><strong>Mutual Information</strong> \( I(X, Y) \):
\[
I(X, Y) = \sum_{x \in X, y \in Y} p(X = x, Y = y) \log \left( \frac{p(X = x, Y = y)}{p(X = x) p(Y = y)} \right)
\]
Measures how much knowing \( X \) reduces uncertainty about \( Y \).</p>
</li>
<li>
<p><strong>Hilbert-Schmidt Independence Criterion (HSIC)</strong>:
\[
\text{HSIC}(X, Y) \propto \text{trace}(KHLH)
\]</p>
<ul>
<li>\( K \): kernel matrix on \( X \).</li>
<li>\( L \): kernel matrix on \( Y \).</li>
<li>\( H \): centering matrix.</li>
<li>HSIC measures dependence between two variables in a kernel space.</li>
</ul>
</li>
</ol>
<hr />
<h2 id="submodular-functions"><a class="header" href="#submodular-functions"><strong>Submodular Functions</strong></a></h2>
<ul>
<li>A set function \( q \) is <strong>submodular</strong> if it satisfies diminishing returns:
\[
q(S \cup X) - q(S) \geq q(T \cup X) - q(T), \quad \text{for } S \subseteq T
\]</li>
<li><strong>Greedy Near-Optimality</strong>: If \( q \) is submodular and non-decreasing, greedy selection guarantees at least 63% of the optimal solution:
\[
q(S) \geq (1 - \frac{1}{e}) \max_{|T| = |S|} q(T)
\]</li>
</ul>
<hr />
<h2 id="wrapper-methods"><a class="header" href="#wrapper-methods"><strong>Wrapper Methods</strong></a></h2>
<ul>
<li>
<p><strong>Not Embedded</strong>: Use a learning algorithm as a quality measure for feature sets.</p>
<ul>
<li><strong>Simple Wrapper</strong>: Apply a classifier to each feature and evaluate its quality.</li>
<li>Extend to groups of features with <strong>heuristic search</strong> (greedy, Monte Carlo, etc.).</li>
</ul>
</li>
<li>
<p><strong>Embedded Methods</strong>: Feature selection is integrated into the learning algorithm.</p>
<ul>
<li>Example: <strong>\( \ell_0 \)-norm SVM</strong> iterates between feature re-scaling and SVM training.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="probe-method-for-determining-the-number-of-features"><a class="header" href="#probe-method-for-determining-the-number-of-features"><strong>Probe Method</strong> for Determining the Number of Features</a></h2>
<ul>
<li><strong>Problem</strong>: Random features may show significant correlations, leading to false positives.</li>
<li><strong>Solution</strong>: Insert fake (random) features and stop when the first fake feature is selected.</li>
</ul>
<hr />
<h2 id="unsupervised-feature-selection"><a class="header" href="#unsupervised-feature-selection"><strong>Unsupervised Feature Selection</strong></a></h2>
<ul>
<li><strong>No Target Variable</strong>: Select features that are informative based on specific criteria such as:
<ul>
<li><strong>Saliency</strong>: Features with high variance.</li>
<li><strong>Entropy</strong>: Features with a uniform distribution of values.</li>
<li><strong>Smoothness</strong>: Features with moderate curvature in time series data.</li>
<li><strong>Density</strong>: Features connected to many other variables.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="feature-selection-in-practice"><a class="header" href="#feature-selection-in-practice"><strong>Feature Selection in Practice</strong></a></h2>
<ul>
<li><strong>10 Questions from Guyon and Elisseeff</strong>:
<ol>
<li>Do you have domain knowledge?</li>
<li>Are the features commensurate (measurable on the same scale)?</li>
<li>Do you suspect feature interdependence?</li>
<li>Do you need to prune the feature set?</li>
<li>Should features be ranked individually?</li>
<li>Do you need a predictor?</li>
<li>Is your data “dirty”?</li>
<li>What should you try first (linear predictors, forward selection, etc.)?</li>
<li>Do you have the resources to test multiple methods?</li>
<li>Do you want a stable solution (e.g., via bootstrapping)?</li>
</ol>
</li>
</ul>
<hr />
<h2 id="revealing-examples"><a class="header" href="#revealing-examples"><strong>Revealing Examples</strong></a></h2>
<ul>
<li><strong>Redundancy</strong>: Highly correlated features are redundant, but they can still provide complementary information.</li>
<li><strong>Collaborative Variables</strong>: Two variables that are individually irrelevant can become important when considered together.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="applications-in-computational-biology"><a class="header" href="#applications-in-computational-biology">Applications in Computational Biology</a></h1>
<h2 id="deleteriousness-prediction"><a class="header" href="#deleteriousness-prediction"><strong>Deleteriousness Prediction</strong></a></h2>
<ul>
<li>
<p><strong>Objective</strong>: Assess whether a genetic variant, specifically a missense variant (which causes amino acid changes), is deleterious (harmful).</p>
</li>
<li>
<p><strong>Challenges</strong>: Tens of thousands of variants may exist in a patient’s genome, necessitating computational tools for prediction.</p>
</li>
<li>
<p><strong>Popular Tools</strong>:</p>
<ul>
<li><strong>SIFT</strong>, <strong>PolyPhen</strong>, <strong>MutationTaster</strong>, <strong>GERP</strong>, <strong>FatHMM</strong>, among others, are widely used to predict deleteriousness.</li>
</ul>
</li>
<li>
<p><strong>Issues with Current Methods</strong>:</p>
<ul>
<li><strong>Type 1 Circularity</strong>: Benchmark datasets used for both training and testing tools overlap significantly.</li>
<li><strong>Type 2 Circularity</strong>: Proteins often contain only deleterious or neutral variants, leading to artificially high accuracy via majority vote.</li>
</ul>
</li>
<li>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Cleanly separate training and test datasets to avoid circularity.</li>
<li>Stratify datasets by protein membership.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="phenotype-prediction-and-epistasis"><a class="header" href="#phenotype-prediction-and-epistasis"><strong>Phenotype Prediction and Epistasis</strong></a></h2>
<ul>
<li>
<p><strong>Goal</strong>: Predict phenotypic traits (observable characteristics) from an individual's genotype (genetic makeup).</p>
</li>
<li>
<p><strong>Genome-Wide Association Studies (GWAS)</strong>: Analyze genome-wide genetic variations to find associations with phenotypes.</p>
</li>
<li>
<p><strong>Recent Work</strong>:</p>
<ul>
<li>Example from <strong>Vilain Lab</strong> (UCLA): Claimed that specific methylation patterns in twins could predict sexual orientation with 70% accuracy, but criticisms included small sample size and overfitting.</li>
</ul>
</li>
<li>
<p><strong>Lessons</strong>:</p>
<ol>
<li><strong>Low sample sizes</strong> still hinder predictions of complex traits.</li>
<li><strong>Overfitting</strong> must be avoided by building models that generalize well to unseen data.</li>
<li>Correcting for <strong>multiple testing</strong> is crucial in high-dimensional spaces to avoid false positives.</li>
</ol>
</li>
</ul>
<hr />
<h2 id="epistasis-gene-gene-interactions"><a class="header" href="#epistasis-gene-gene-interactions"><strong>Epistasis (Gene-Gene Interactions)</strong></a></h2>
<ul>
<li>
<p><strong>Definition</strong>: Epistasis refers to the interaction between genes where the effect of one gene is modified by one or more other genes.</p>
</li>
<li>
<p><strong>Types</strong>:</p>
<ul>
<li><strong>Bateson's Masking Model</strong>: One gene masks the effect of another gene.</li>
<li><strong>General Epistasis</strong>: More complex interactions between two loci.</li>
</ul>
</li>
<li>
<p><strong>Models for Epistasis</strong>:</p>
<ol>
<li><strong>Multiplicative Interaction</strong>: Odds increase multiplicatively with certain genotypes.</li>
<li><strong>Threshold Model</strong>: Interaction only manifests when both loci contain disease-associated alleles.</li>
</ol>
</li>
<li>
<p><strong>Applications</strong>: Epistasis is often cited as one explanation for the missing heritability of complex traits, such as breast cancer, where gene interactions affect disease risk.</p>
</li>
</ul>
<hr />
<h2 id="bottlenecks-in-two-locus-mapping"><a class="header" href="#bottlenecks-in-two-locus-mapping"><strong>Bottlenecks in Two-Locus Mapping</strong></a></h2>
<ul>
<li>
<p><strong>Scale</strong>: The large number of single nucleotide polymorphisms (SNPs), typically \( 10^5 - 10^7 \), leads to considering an enormous number of SNP pairs (~\( 10^{10} - 10^{14} \)).</p>
</li>
<li>
<p><strong>Challenges</strong>:</p>
<ul>
<li>Multiple hypothesis testing.</li>
<li>Long computational runtimes.</li>
</ul>
</li>
<li>
<p><strong>Approaches</strong>:</p>
<ul>
<li><strong>Exhaustive Enumeration</strong>: Requires specialized hardware like GPUs.</li>
<li><strong>Filtering Methods</strong>: Prioritize SNPs based on statistical criteria (e.g., large main effects) or biological criteria (e.g., protein-protein interactions).</li>
</ul>
</li>
</ul>
<hr />
<h2 id="conclusion-and-future-directions"><a class="header" href="#conclusion-and-future-directions"><strong>Conclusion and Future Directions</strong></a></h2>
<ul>
<li>Data mining techniques in computational biology have advanced significantly, providing methods for predicting deleteriousness, phenotypic traits, and uncovering gene interactions.</li>
<li>Major challenges still include avoiding overfitting, managing small sample sizes, and handling the computational burden of analyzing vast genetic datasets.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mobile-health-and-activity-monitoring"><a class="header" href="#mobile-health-and-activity-monitoring">Mobile Health and Activity Monitoring</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="big-data-for-engineers"><a class="header" href="#big-data-for-engineers">Big Data for Engineers</a></h1>
<ul>
<li><a href="23fs/bdfe/01_azure_blob_storage.html">01. Azure Blob Storage</a></li>
<li><a href="23fs/bdfe/02_hadoop.html">02. Hadoop</a></li>
<li><a href="23fs/bdfe/03_spark.html">03. Spark</a></li>
<li><a href="23fs/bdfe/04_jsoniq.html">04. Jsoniq</a></li>
<li><a href="23fs/bdfe/05_hbase_(wide_column_store).html">05. Hbase (wide Column Store)</a></li>
<li><a href="23fs/bdfe/06_mapreduce.html">06. Mapreduce</a></li>
<li><a href="23fs/bdfe/07_yarn.html">07. Yarn</a></li>
</ul>
<!-- toc -->
<div style="break-before: page; page-break-before: always;"></div><h1 id="azure-blob-storage"><a class="header" href="#azure-blob-storage">Azure Blob Storage</a></h1>
<p>Azure Storage uses a combination of <em>account ID</em>, <em>partition ID</em> and <em>blob ID</em> to uniquely identify a blob. (S3 uses <em>bucket ID</em> and <em>object ID</em>.)</p>
<h2 id="storage-stamp"><a class="header" href="#storage-stamp">Storage Stamp</a></h2>
<p>On the physical level, Azure Blob Storage is organized in
<strong>storage stamp</strong>s located in various data centers worldwide.</p>
<p>Each storage stamp consists of 10 to 20 racks, with each rack containing around 18 storage nodes (the disks + servers).</p>
<p>In all, a storage stamp can store up to ca. 30 PB of data.</p>
<p>However, a storage stamp will not be filled more than 80% of its total capacity in order to avoid being full: if a storage stamp reaches capacity, then some data is going to be reallocated to another storage stamp in the background. And if there are not enough storage stamps, well new racks will need to be purchased and installed at the locations that make the most sense.</p>
<h2 id="types-of-blobs"><a class="header" href="#types-of-blobs">Types of blobs</a></h2>
<p>3 types of blobs: <em>block blobs</em>, <em>append blobs</em>, <em>page blobs</em>.</p>
<p>Type of blob cannot be changed after creation.</p>
<p>All blobs reflect committed changes immediately. Versioning is maintained via <em>ETag</em>.</p>
<h3 id="block-blobs"><a class="header" href="#block-blobs">Block Blobs</a></h3>
<p>Optimized for uploading large amounts of data efficiently.</p>
<p>Can be updated only at the granularity of an entire block.</p>
<h3 id="page-blobs"><a class="header" href="#page-blobs">Page blobs</a></h3>
<p>A collection of 512-byte pages optimized for <em>random read and write operations</em>.</p>
<h3 id="append-blobs"><a class="header" href="#append-blobs">Append blobs</a></h3>
<p>Composed of blocks like block blobs, but are optimized for <em>append operations</em>.</p>
<p>Blocks are added to the end of the blob only.</p>
<p>An append blob <strong>does not expose its block IDs</strong>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hadoop"><a class="header" href="#hadoop">Hadoop</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="spark"><a class="header" href="#spark">Spark</a></h1>
<h2 id="rdd"><a class="header" href="#rdd">RDD</a></h2>
<p><strong>R</strong>esilient <strong>D</strong>istributed <strong>D</strong>ataset.</p>
<ul>
<li><strong>Resilient</strong>: remain in <u>memory</u> or on disk on a &quot;best effort&quot; basis, can be recomputed if needs be</li>
<li><strong>Distributed</strong></li>
</ul>
<p>RDDs <u>need not be</u> collection of key-value pairs, it can be (ordered) collection of <em>anything</em>. But we do have one constraint: <u>the values within the same RDD share the same static type</u>.</p>
<h2 id="rdd-lifecycle"><a class="header" href="#rdd-lifecycle">RDD Lifecycle!!!</a></h2>
<ol>
<li>Creation</li>
<li>Transformation: transform one RDD into another RDD!</li>
<li><strong>Action</strong>
<ol>
<li>A final action that make an output <u>persistent</u>!</li>
</ol>
</li>
<li><strong>Lazy</strong> evaluation
<ol>
<li><u>Creation and transformations on their own do NOTHING</u></li>
<li>Only with an <u>action</u> that the entire computation pipeline is put into motion</li>
</ol>
</li>
</ol>
<h2 id="types-of-transformations"><a class="header" href="#types-of-transformations">Types of transformations</a></h2>
<h3 id="unary"><a class="header" href="#unary">Unary</a></h3>
<ul>
<li><code>filter</code>
<ul>
<li>input: a predicate function: take a value, return a <code>Boolean</code></li>
<li>return: subset of input satisfying the predicate</li>
<li><u>preserve relative order</u></li>
</ul>
</li>
<li><code>map</code>
<ul>
<li>input: a function: take a value, return another value (<strong>one to one!!!</strong>)</li>
<li>return: the list of values obtained by applying this function to <u>each value in the input</u></li>
</ul>
</li>
<li><code>flatMap</code>:
<ul>
<li>input: a function: take a value, return <strong>0, 1, or more</strong> values</li>
<li>return: the list of values obtained by applying this function to <u>each value in the input</u>, <strong>flattening the obtained values</strong> (information on which values came from the same input value is <u>lost</u>)</li>
<li><strong><code>flatMap</code> IS THE SAME AS MapReduce's map, not <code>map</code>!!!</strong></li>
</ul>
</li>
<li><code>distinct</code>
<ul>
<li>input: comparator function (or none if the values are comparable)</li>
<li>return: the list of values obtained by removing duplicates from the input</li>
</ul>
</li>
<li><code>sample</code>
<ul>
<li>input: none additional (just the input RDD)</li>
<li>return: a subset of the input RDD (smaller than the input RDD)</li>
</ul>
</li>
</ul>
<h3 id="binary"><a class="header" href="#binary">Binary</a></h3>
<ul>
<li><code>union</code></li>
<li><code>intersection</code></li>
<li><code>subtract</code>: remove all elements from the first RDD (left) that are also in the second RDD (right)</li>
</ul>
<h3 id="pair-transformations"><a class="header" href="#pair-transformations">Pair transformations</a></h3>
<p>Spark has transformations specifically tailored for RDDs of key-value pairs!</p>
<ul>
<li><code>key</code>
<ul>
<li>return only the keys of the input RDD</li>
</ul>
</li>
<li><code>values</code>
<ul>
<li>return only the values of the input RDD</li>
</ul>
</li>
<li><code>reduceByKeys</code>
<ul>
<li>input: a (normally associative and commutative) binary operator</li>
<li>return: a new RDD with the same keys as the input RDD, but with the values reduced by the binary operator (invokes and chians this operator on all values of the input RDD that share the same key)
<ul>
<li>(k, (v1 + v2 + ... + vn)) is output assuming + is the operator.</li>
</ul>
</li>
<li><strong><code>reduceByKey</code> IS THE SAME AS MapReduce's reduce!!!</strong></li>
</ul>
</li>
<li><code>groupByKey</code>
<ul>
<li>groups all kv pairs by key, and returns a single kv for each key where value is an array</li>
</ul>
</li>
<li><code>sortByKey</code></li>
<li><code>mapValues</code>
<ul>
<li>Similar to the <code>map</code> transformation (not <code>flatMap</code>!), but map function only applied to the value and the key is kept</li>
</ul>
</li>
<li><code>join</code>
<ul>
<li>works on two input RDDs <u>or</u> key-value pairs</li>
<li>matches the pairs on both sides that have the same <strong>key</strong></li>
<li>outputs, for <strong>each</strong> match, <strong>an output pair</strong> with that shared key and a <strong>tuple</strong> with the <strong>two values</strong> from each side.</li>
<li>If there are multiple values with the same key on any side (or both), then all possible combinations are output.</li>
</ul>
</li>
<li><code>subtractByKey</code></li>
</ul>
<h2 id="types-of-actions"><a class="header" href="#types-of-actions">Types of actions!</a></h2>
<h3 id="gather-output-locally"><a class="header" href="#gather-output-locally">Gather output locally</a></h3>
<p>By locally we mean in the client machine memory!</p>
<ul>
<li><code>collect</code>
<ul>
<li>downloads all values of an RDD on the client machine and output as a local list</li>
<li><strong>only use if the output is small enough to fit in memory</strong></li>
</ul>
</li>
<li><code>count</code>
<ul>
<li>computes (in parallel) the total number of values (count duplicates!) in the input RDD</li>
<li><em>safe for large RDDs bcuz only returns a smol integer</em></li>
</ul>
</li>
<li><code>countByValue</code>
<ul>
<li>computes (in parallel) the number of times each <u>distinct</u> value appears in the input RDD</li>
<li><strong>only use if the output is small enough to fit in memory</strong></li>
</ul>
</li>
<li><code>take</code>
<ul>
<li>returns the <strong>first</strong> <code>n</code> values of the input RDD</li>
</ul>
</li>
<li><code>top</code>
<ul>
<li>returns the <strong>last</strong> <code>n</code> values of the input RDD</li>
</ul>
</li>
<li><code>takeSample</code>
<ul>
<li>returns a random sample of <code>n</code> values from the input RDD</li>
</ul>
</li>
<li><code>reduce</code>
<ul>
<li>input: a (normally associative and commutative) binary operator</li>
<li>return: a new RDD with the operator invoked and chained on all values of the input RDD
<ul>
<li>(v1 + v2 + ... + vn if + is the operator) and outputs the resulting value.</li>
</ul>
</li>
<li>no key!</li>
<li>output is a single value?</li>
</ul>
</li>
</ul>
<h3 id="write-output"><a class="header" href="#write-output">Write output</a></h3>
<ul>
<li><code>saveAsTextFile</code></li>
<li><code>saveAsObjectFile</code></li>
</ul>
<h3 id="actions-for-pair-rdds"><a class="header" href="#actions-for-pair-rdds">Actions for Pair RDDs</a></h3>
<ul>
<li><code>countByKey</code>
<ul>
<li>outputs locally each key together with the number of values in the input taht are associated with this key</li>
<li>a local list of key-value pairs</li>
<li><strong>only use if the the input RDD does not have lots of distinct keys</strong></li>
</ul>
</li>
<li><code>lookup</code>
<ul>
<li>get the value or values associated with a given key</li>
</ul>
</li>
</ul>
<h2 id="physical-architecture"><a class="header" href="#physical-architecture">Physical Architecture</a></h2>
<ul>
<li>narrow-dependency: computation involves <u>only a single input</u></li>
<li>wide-dependency: computation involves multiple inputs</li>
</ul>
<p><strong>Stage</strong>: chain of narrow dependency transformations (<code>map</code>, <code>filter</code>, <code>flatMap</code>) etc (== phase in MapReduce)</p>
<p><img src="23fs/bdfe/img/20230823174842.png" alt="Relationship between transformation, stage, task and job" /></p>
<h2 id="optimization"><a class="header" href="#optimization">Optimization</a></h2>
<ul>
<li><strong>Pinning RDDs</strong>
<ul>
<li>Everytime an <u>action is triggered</u>, all the computations of the &quot;reverse transitive closure&quot; (i.e. all theway up the DAG thru the reverted edges) are set into motion.</li>
<li>The intermediate RDDs in the <strong>shared subgraph</strong> is worthy to be <strong>pinned</strong> (<strong>persisted</strong>) in memory and/or on disk.</li>
</ul>
</li>
<li><strong>Pre-partitioning</strong>
<ul>
<li>If <code>Spark</code> knows that the data is <em>already</em> located where it should be, it will not perform shuffle</li>
<li>Example: when data is sorted before being grouped with the same keys after sorting</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="jsoniq"><a class="header" href="#jsoniq">JSONiq</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hbase-wide-column-store"><a class="header" href="#hbase-wide-column-store">HBase (Wide Column Store)</a></h1>
<h2 id="hbase-commands"><a class="header" href="#hbase-commands"><code>HBase</code> commands</a></h2>
<p><code>HBase</code> supports four kinds of low-level queries: <code>get</code>, <code>put</code>, <code>scan</code> and <code>delete</code>. Unlike a traditional key-value store, HBase also supports querying ranges of row IDs and ranges of timestamps.</p>
<pre><code class="language-bash">put '&lt;name_space:table_name&gt;', '&lt;row_key&gt;' '&lt;cf:column_name&gt;', '&lt;value&gt;'
</code></pre>
<p><strong>Table name</strong> and <strong>column family (cf)</strong> must be known in advance.</p>
<h2 id="hfile"><a class="header" href="#hfile"><code>HFile</code></a></h2>
<p>A flat list of KeyValues, one per <em>cell</em> in the table.
The KeyValues are sorted (first by row ID, then by column family, then by column qualifier, then by version (recent to old)).</p>
<p>This means <u>all versions of a give cell that are in the same <code>HFile</code> are located together</u>.</p>
<p>The KeyValues within an <code>HFile</code> are <u>organized in blocks</u> called <code>HBlocks</code>. They have a size of 64kB but if the last KeyValue is larger than 64kB, then the block will be larger.</p>
<p>The <code>HFile</code> also contains an index of all blocks with their key boundaries. The index is loaded in memory prior to reading anything from the <code>HFile</code> and is kept in memory to speed up reads.</p>
<h3 id="log-structured-merge-trees"><a class="header" href="#log-structured-merge-trees">Log-structured merge trees</a></h3>
<p><code>HBase</code> first store cells in memory (<code>MemStore</code>) as long as there is enough memory available. Once the memory is full, the <code>MemStore</code> is flushed to disk as an <code>HFile</code>. Upon flushing, all cells are written sequentially to a new HFile in ascending key order, HBlock by HBlock, concurrently building the index structure.</p>
<p>After many flushes, the number of <code>HFile</code>s to read from grows and
becomes impracticable. For this reason, there is an additional process called compaction that takes several <code>HFiles</code> And outputs a single, merged <code>HFile</code>. Since the cells within each <code>HFile</code> are already sorted, this can be done in linear time, as this is essentially the merge part of the merge-sort algorithm.</p>
<p><strong>The merge happens like the game 2048!!</strong></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mapreduce"><a class="header" href="#mapreduce">MapReduce</a></h1>
<p>In MapReduce, the input data, intermediate data, and output data are all made of a large collection of <strong>key-value pairs</strong> (with the keys not necessarily unique, and not necessarily sorted by key)</p>
<p>The types of the keys and values are known at compile-time (<u>statically</u>), and they <u>do not need to be the same across all three collections</u>.</p>
<h2 id="combine"><a class="header" href="#combine">Combine</a></h2>
<p>In addition to the map function and the reduce function, the
user can supply a combine function. This combine function can then be called by the system during the map phase as many times as it sees fit to “compress” the intermediate key-value pairs.</p>
<p>Strategically, the combine function is likely to be called at every flush of key-value pairs to a Sequence File on disk, and at every compaction of several Sequence Files into one.</p>
<p>However, there is no guarantee that the combine function will becalled at all, and there is also no guarantee on how many times it will be called. Thus, if the user provides a combine function, it is important that they think carefully about a combine function that does not affect the correctness of the output data.</p>
<p>In fact, in most of the cases, the combine function will be identical to the reduce function, which is generally possibly if the intermediate key-value pairs have the same type as the output key-value pairs, and the reduce function is both associative and commutative. This is the case for summing values as well as for taking the maximum or the minimum, but not for an unweighted average (why?). As a reminder, associativity means that \( (a +b)+c = a +(b +c) \) and commutativity means that \( a +b = b +a \).</p>
<h2 id="terms"><a class="header" href="#terms">Terms!!</a></h2>
<h3 id="function"><a class="header" href="#function">&quot;function&quot;</a></h3>
<p>A map function is a mathematical, or programmed, function that
takes <strong>one</strong> input key-value pair and returns <strong>zero</strong>, <strong>one</strong> or <strong>more</strong> intermediate key-value pairs.</p>
<p>A reduce function is a mathematical, or programmed, function that
takes <strong>one or more</strong> intermediate key-value pairs and returns <strong>zero</strong>, <strong>one</strong> or <strong>more</strong> output key-value pairs.</p>
<p>A combine function is a mathematical, or programmed, function
that takes <strong>one or more</strong> intermediate key-value pairs and returns <strong>zero</strong>, <strong>one</strong> or <strong>more</strong> intermediate key-value pairs.</p>
<h3 id="task"><a class="header" href="#task">&quot;task&quot;</a></h3>
<p>A map <strong>task</strong> is an assignment (or “homework”, or “TODO”) that consists in a (<strong>sequential</strong>) series of calls of the <u>map function</u> on a subset of the input. There is <u>one map task for every input split</u>, so that there are many map tasks as partitions of the input.</p>
<p>A reduce task is an assignment that consists in a (<strong>sequential</strong>) series
of calls of the <u>reduce function</u> on a subset of the intermediate input. There are as many reduce tasks as <u>partitions of the list of intermediate key-value pairs.</u></p>
<p><strong>There is no parallelism at all within a task</strong>!!!</p>
<p>Calls of the combine function are not planned as a task, but is called ad-hoc during flushing and compaction.</p>
<h3 id="slots"><a class="header" href="#slots">&quot;slots&quot;</a></h3>
<p>Resources (CPU and RAM) used to process one or more tasks.</p>
<p><strong>There is no parallelism within a slot</strong>!!!</p>
<h3 id="phase"><a class="header" href="#phase">&quot;phase&quot;</a></h3>
<p>The map phase thus consists of several map slots processing map tasks in parallel, and the reduce phase consists of several reduce slots processing reduce tasks in parallel.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="yarn"><a class="header" href="#yarn">YARN</a></h1>
<h2 id="mapreduce-architecture-version-1-no-yarn"><a class="header" href="#mapreduce-architecture-version-1-no-yarn">MapReduce architecture version 1 (no YARN)</a></h2>
<p>Per cluster 1 <code>JobTracker</code> (typically run together on <code>NameNode</code> and <code>HMaster</code> )+ multiple <code>TaskTracker</code>s (typically run together on <code>DataNode</code> and <code>RegionServer</code>).</p>
<p>Intermediate pairs are sorted by key and flushed to disk to a Sequence File (will be merged according to log-structured merge trees).</p>
<p>When the map phase is over, each TaskTracker runs an HTTP server
listening for connections, so that they can connect to each other and ship the intermediate data over to create the intermediate partitions ensuring that <em>the same keys are on the same machines</em></p>
<p>shuffling can start before the map phase is over, but the
reduce phase can only start after the map phase is over</p>
<p>reduce phase outputs <strong>shard</strong>s (file names are <code>part-00000</code>, <code>part-00001</code>, etc) to the distributed file system (HDFS, S3 etc)</p>
<p>In the very first version of MapReduce (with a JobTracker and
TaskTrackers), map slots and reduce slots are all <strong>pre-allocated</strong> from the very beginning, which blocks parts of the cluster remaining idle in both phases.</p>
<h2 id="yarn-general-architecture"><a class="header" href="#yarn-general-architecture">YARN general architecture</a></h2>
<p>not <code>JobTracker</code> and <code>TaskTracker</code> anymore, but <code>ResourceManager</code> and <code>NodeManager</code>. <code>NodeManager</code>s provide &quot;slots&quot; as containers for tasks.</p>
<p>YARN provides generic support for allocating resources to any application and is application-agnostic. When a new application is launched, the <strong>ResourceManager assigns one of the container to act as the ApplicationMaster</strong> which will take care of running the application. This is a fundamental change from the initial MapReduce architecture, in which the JobTracker was <em>also</em> taking care of running the MapReduce job. The ApplicationMaster can then communicate with the ResourceManager in order to book and use more containers in order to run jobs.</p>
<p><img src="23fs/bdfe/img/20230823191134.png" alt="Application Manager" /></p>
<p>YARN cleanly separates between the general management of
resources and bootstrapping new applications, which remains centralized on the coordinator node, and monitoring the job lifecycle, which
is now delegated to one or more ApplicationMasters running concurrently. This means, in particular, that several applications can run concurrently in the same cluster. This ability, known as multi-tenancy, is very important for large companies or universities in order to optimize the use of their resources.</p>
<h2 id="resource-management"><a class="header" href="#resource-management">Resource management</a></h2>
<p>By issuing tokens.</p>
<p>Bootstrap a new application: <code>ResourceManager</code> issues <strong>application tokens</strong> to clients and start the <code>ApplicationMaster</code>.</p>
<p>General resource request:</p>
<ol>
<li><code>ApplicationMaster</code> request container with spec from <code>ResourceManager</code> (e.g. 10core 10GB RAM)</li>
<li><code>ResourceManager</code> allocates  issues <strong>container tokens</strong> to <code>ApplicationMaster</code></li>
<li><code>ApplicationMaster</code> connects to the allocated <code>NodeManager</code> and sends the container token</li>
<li><code>NodeManager</code> checks the container token and starts the container</li>
<li><code>ApplicationMaster</code> ships the code and parameters to the container</li>
</ol>
<h2 id="scheduling"><a class="header" href="#scheduling">Scheduling!</a></h2>
<p>The ResourceManager decides whether and when to grant resource requests based on several factors: capacity guarantees, fairness, service level agreements (remember the numbers with plenty of 9s?) and with the <strong>goal</strong> to <strong>maximize cluster utilization</strong></p>
<p>The ResourceManager keeps track of the list of available NodeManagers (who can dynamically come and go) and their status. Just like in HDFS, NodeManagers send periodic heartbeats to the ResourceManager to give a sign of life.</p>
<h3 id="strategies"><a class="header" href="#strategies">Strategies</a></h3>
<ul>
<li>FIFO</li>
<li>Capacity
<ul>
<li>different sizes of subclusters</li>
<li>can have hierarchical subclusters (hierarchical queues)</li>
<li>can lend resources to other subclusters if not used</li>
</ul>
</li>
<li>Fair scheduling -- compute cluster shares dynamically
<ul>
<li>Steady fair share
<ul>
<li>share of the cluster officially allocated to each user</li>
<li>user agree on the share in advance</li>
<li>static, rarely change</li>
</ul>
</li>
<li>Instantaneous fair share
<ul>
<li>the fair share that a department should ideally be allocated</li>
<li>changes constantly</li>
<li>if user is idle, then the instantaneous fair share of other user becomes larger</li>
</ul>
</li>
<li>Current share
<ul>
<li>Actual share of the cluster</li>
<li>highly dynamic</li>
<li>does not necessarily match the instantaneous fair share</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>easiest fair scheduling: the requests by users who are significantly below their instantaneous fair share are prioritized</p>
<p>Multidimensional resource scheduling: <strong>Dominant Resource Fairness algorithm</strong></p>
<p>e.g. 1000 cores and 10TB mem cluster, A requests containers with 1 core and 100GB RAM, B requests containers with 4 cores and 10GB RAM</p>
<ul>
<li>A: 0.1% core, 1% mem -- dominant resource is mem</li>
<li>B: 0.4% core, 0.1% mem -- dominant resource is core</li>
</ul>
<p>Say they both have instantaneous fair share of 50%, then every time A gets 2 containers, B gets 5 containers.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="functional-genomics"><a class="header" href="#functional-genomics">Functional Genomics</a></h1>
<p>A course collectively taught by multiple professors. Multiple-choice exam, lots of memorization.</p>
<h2 id="table-of-contents-1"><a class="header" href="#table-of-contents-1">Table of Contents</a></h2>
<ul>
<li><a href="23fs/fg/01_modern_genomics_i.html">01. Modern Genomics I</a></li>
<li><a href="23fs/fg/02_modern_genomics_ii.html">02. Modern Genomics II</a></li>
<li><a href="23fs/fg/03_transcriptomics_i.html">03. Transcriptomics I</a></li>
<li><a href="23fs/fg/04_transcriptomics_ii.html">04. Transcriptomics II</a></li>
<li><a href="23fs/fg/05_mirnas_and_other_small_rnas.html">05. miRNAs and Other Small RNAs</a></li>
<li><a href="23fs/fg/06_proteomics.html">06. Proteomics</a></li>
<li><a href="23fs/fg/07_metabolomics.html">07. Metabolomics</a></li>
<li><a href="23fs/fg/08_single_cell_mass_cytometry.html">08. Single Cell Mass Cytometry</a></li>
<li><a href="23fs/fg/09_protein_networks.html">09. Protein Networks</a></li>
<li><a href="23fs/fg/10_epigenomics_and_gene_regulation.html">10. Epigenomics and Gene Regulation</a></li>
<li><a href="23fs/fg/11_quality_control_and_standards.html">11. Quality Control and Standards</a></li>
</ul>
<!-- toc -->
<div style="break-before: page; page-break-before: always;"></div><h1 id="modern-genomics-i"><a class="header" href="#modern-genomics-i">Modern Genomics I</a></h1>
<ul>
<li><a href="23fs/fg/01_modern_genomics_i.html#introduction">Introduction</a>
<ul>
<li><a href="23fs/fg/01_modern_genomics_i.html#timeline">Timeline</a></li>
<li><a href="23fs/fg/01_modern_genomics_i.html#why-genomics">Why Genomics?</a></li>
<li><a href="23fs/fg/01_modern_genomics_i.html#comparative-genomics-use-case-examples">Comparative genomics use case examples</a></li>
</ul>
</li>
<li><a href="23fs/fg/01_modern_genomics_i.html#classical-sequencing-method">Classical sequencing method</a>
<ul>
<li><a href="23fs/fg/01_modern_genomics_i.html#sanger-double-deoxy-sequencing">Sanger (double-deoxy) sequencing</a></li>
<li><a href="23fs/fg/01_modern_genomics_i.html#automated-dye-sequencing">Automated Dye Sequencing</a></li>
</ul>
</li>
<li><a href="23fs/fg/01_modern_genomics_i.html#new-next-generation-sequencing-technologies">New (Next-generation) sequencing technologies</a>
<ul>
<li><a href="23fs/fg/01_modern_genomics_i.html#amplification-technologies">Amplification technologies</a>
<ul>
<li><a href="23fs/fg/01_modern_genomics_i.html#emulsion-pcr">Emulsion PCR</a></li>
<li><a href="23fs/fg/01_modern_genomics_i.html#pcr-on-solid-support">PCR on solid support</a></li>
</ul>
</li>
<li><a href="23fs/fg/01_modern_genomics_i.html#barcoding-and-linked-reads">Barcoding and &quot;linked reads&quot;</a></li>
<li><a href="23fs/fg/01_modern_genomics_i.html#sequencing-technologies">Sequencing technologies</a>
<ul>
<li><a href="23fs/fg/01_modern_genomics_i.html#pyrosequencing">Pyrosequencing</a></li>
<li><a href="23fs/fg/01_modern_genomics_i.html#reversible-terminator-sequencing">Reversible terminator sequencing</a></li>
<li><a href="23fs/fg/01_modern_genomics_i.html#sequencing-by-semi-conductor">Sequencing by semi-conductor</a></li>
</ul>
</li>
<li><a href="23fs/fg/01_modern_genomics_i.html#current-implementations-of-ngs">Current implementations of NGS</a></li>
</ul>
</li>
<li><a href="23fs/fg/01_modern_genomics_i.html#third-generation-sequencing-technologies">Third-generation sequencing technologies</a></li>
<li><a href="23fs/fg/01_modern_genomics_i.html#environmental-sequencing">Environmental sequencing</a>
<ul>
<li><a href="23fs/fg/01_modern_genomics_i.html#how-to-deal-with-environmental-sequencing-data">How to deal with environmental sequencing data</a></li>
</ul>
</li>
<li><a href="23fs/fg/01_modern_genomics_i.html#single-cell-sequencing">Single-cell sequencing</a>
<ul>
<li><a href="23fs/fg/01_modern_genomics_i.html#why">Why?</a></li>
<li><a href="23fs/fg/01_modern_genomics_i.html#how">How?</a>
<ul>
<li><a href="23fs/fg/01_modern_genomics_i.html#single-cell-isolation">Single-cell isolation</a></li>
<li><a href="23fs/fg/01_modern_genomics_i.html#whole-genome-amplification">Whole genome amplification</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="23fs/fg/01_modern_genomics_i.html#genomic-databases">Genomic databases</a></li>
</ul>
<h2 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h2>
<p><span style="color:gray">Molecular biology is governed by the &quot;central dogma&quot;. Genomics is basically studying the <em>DNA</em> part of it.</span></p>
<p><strong>Genome sequence</strong>: complete listing of <u>all nucleotides</u> of <u>one organisms</u>, in correct order, and mapped to the chromosomes.</p>
<pre class="mermaid">graph LR
    A[DNA] --&gt;|Transcription| B[RNA]
    B --&gt;|Translation| C[Protein]
</pre>
<h3 id="timeline"><a class="header" href="#timeline">Timeline</a></h3>
<p>Efficient sequencing technology arrived rather late. Initially the sequencing process was cumbersome and radioactive.</p>
<ul>
<li>1975: &quot;dideoxy&quot; DNA sequencing (Sanger)</li>
<li>1977: first genome (<u>bacteriophage \( \phi X 174 \) </u>)</li>
<li>1995: first <u>cell</u> (Haemophilus influenzae)</li>
<li>1998: first animal (Caenorhabditis elegans)</li>
<li>2001: Homo sapiens</li>
<li>Today (February 2023)
<ul>
<li><span style="color:gray">genomes available for: 409,947 Bacteria, 4,988 Archaea, 47,200 Eukaryotes</span></li>
<li><span style="color:gray">human genomes fairly routine</span>
<ul>
<li><span style="color:gray">below 1000$ raw costs</span></li>
<li><span style="color:gray">&quot;Personal Genome Projects&quot; are enrolling 100’000s of volunteers, including their medical records</span></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="why-genomics"><a class="header" href="#why-genomics">Why Genomics?</a></h3>
<ul>
<li>Because we want an inventory of all genes and functions</li>
<li>Because wea can compare genomes to learn about evolution, to get hints on gene function, etc
<ul>
<li>Comparison can be either based on DNA or protein</li>
<li>Alignments, dot plots, whole chromosome comparison</li>
</ul>
</li>
</ul>
<h3 id="comparative-genomics-use-case-examples"><a class="header" href="#comparative-genomics-use-case-examples">Comparative genomics use case examples</a></h3>
<ul>
<li>Gene prediction
<ul>
<li>Gene prediction algorithms that use homology (=comparative genomics result) information: SLAM, SGP, Twinscan (= N-SCAN)...</li>
<li><img src="23fs/fg/img/20240202163019.png" alt="UCSC genome browser gene prediction" /></li>
</ul>
</li>
<li>Gene family evolution</li>
</ul>
<h2 id="classical-sequencing-method"><a class="header" href="#classical-sequencing-method">Classical sequencing method</a></h2>
<h3 id="sanger-double-deoxy-sequencing"><a class="header" href="#sanger-double-deoxy-sequencing">Sanger (double-deoxy) sequencing</a></h3>
<p>Natural DNA extension requires 3'-OH. The dideoxy method uses a 2',3'-dideoxy nucleotide, which lacks the 3'-OH group. This causes the DNA chain to terminate. By introducing different dideoxy nucleotides, the sequence can be read.</p>
<h3 id="automated-dye-sequencing"><a class="header" href="#automated-dye-sequencing">Automated Dye Sequencing</a></h3>
<p>Variants of Sanger sequencing. Still utilize the dideoxy method to terminate DNA elongation. The difference is that the dideoxy nucleotides are labeled with different fluorescent dyes. The sequence is read by a laser.</p>
<p><img src="23fs/fg/img/20240202163700.png" alt="Two variants of Automated Dye Sequencing" /></p>
<p>Dye terminator sequencing is now widely used over the rather cumbersome (4 tubes per sample) dye primer chemistry.</p>
<h2 id="new-next-generation-sequencing-technologies"><a class="header" href="#new-next-generation-sequencing-technologies">New (Next-generation) sequencing technologies</a></h2>
<p>Generally involves first <strong>amplifying</strong> the DNA, then <strong>sequencing</strong> it. Sequencing is done by detecting the nucleotides as they are incorporated into the growing DNA strand (sequencing by synthesis). High-throughput is achieved by parallelizing the sequencing process.</p>
<h3 id="amplification-technologies"><a class="header" href="#amplification-technologies">Amplification technologies</a></h3>
<p>First-generation amplification technology: needs DNA-library in bacterial vectors --&gt; cumbersome and biased</p>
<p>Improvement: get rid of bacteria</p>
<h4 id="emulsion-pcr"><a class="header" href="#emulsion-pcr">Emulsion PCR</a></h4>
<p>Improvement: bacterium free, but still needs cloning</p>
<pre class="mermaid">%%{init: {&quot;graph&quot;: {&quot;htmlLabels&quot;: false}} }%%
graph TD
    A[&quot;`**Fragment** the DNA, ligate **adapters** to ends, make **single-stranded**`&quot;] --&gt; B[&quot;Attach to microbeads&quot;]
    B --&gt; C[&quot;`PCR-amplify, in a **water-oil emulsion**`&quot;]
    C --&gt; D[&quot;`Enrich beads having successful amplifications, then place into regular lattice
    (see figure below for details)`&quot;]
</pre>
<p><img src="23fs/fg/img/20240203111110.png" alt="Emulsion PCR enrichment step" /></p>
<p>In short, the enrichment is done by capturing the second (5'-end) primer of the PCR product onto a large polysyrene bead.</p>
<h4 id="pcr-on-solid-support"><a class="header" href="#pcr-on-solid-support">PCR on solid support</a></h4>
<p><img src="23fs/fg/img/20240203111302.png" alt="PCR on solid support" /></p>
<h3 id="barcoding-and-linked-reads"><a class="header" href="#barcoding-and-linked-reads">Barcoding and &quot;linked reads&quot;</a></h3>
<p><img src="23fs/fg/img/20240203114453.png" alt="Barcoding" /></p>
<p><img src="23fs/fg/img/20240203114512.png" alt="10X technology" /></p>
<h3 id="sequencing-technologies"><a class="header" href="#sequencing-technologies">Sequencing technologies</a></h3>
<p>First-generation sequencing needs DNA size-separation on a gel</p>
<p>Improvement: get rid of gel (sequencing by synthesis)</p>
<h4 id="pyrosequencing"><a class="header" href="#pyrosequencing">Pyrosequencing</a></h4>
<p><img src="23fs/fg/img/20240203113950.png" alt="Pyrosequencing" /></p>
<h4 id="reversible-terminator-sequencing"><a class="header" href="#reversible-terminator-sequencing">Reversible terminator sequencing</a></h4>
<p><img src="23fs/fg/img/20240203113926.png" alt="Reversible terminator sequencing" /></p>
<h4 id="sequencing-by-semi-conductor"><a class="header" href="#sequencing-by-semi-conductor">Sequencing by semi-conductor</a></h4>
<p>Directly detects the release of H+ ions when a nucleotide is incorporated into the growing DNA strand.</p>
<p><img src="23fs/fg/img/20240203114721.png" alt="Sequencing by semi-conductor" /></p>
<h3 id="current-implementations-of-ngs"><a class="header" href="#current-implementations-of-ngs">Current implementations of NGS</a></h3>
<ul>
<li>Illumina
<ul>
<li>Illumina NovaSeq 6000</li>
<li>PCR on <u>solid support</u></li>
<li><u>reversible terminator</u> sequencing</li>
<li>read length ca. 250bp</li>
<li><code>1e14</code> bp per run</li>
</ul>
</li>
<li>Ion Torrent / Life Techn. Inc
<ul>
<li>Ion Gene Studio S5</li>
<li>PCR on <u>beads</u></li>
<li>sequencing by <u>semi-conductor</u></li>
<li>read length ca. 600bp</li>
<li><code>1e10</code> bp per run</li>
</ul>
</li>
</ul>
<h2 id="third-generation-sequencing-technologies"><a class="header" href="#third-generation-sequencing-technologies">Third-generation sequencing technologies</a></h2>
<p><strong>Single molecule sequencing</strong>. <strong>No</strong> need for <strong>amplification</strong>.</p>
<p>Characterized by extremely long reads, but also high error rates.</p>
<ul>
<li>Pacific Biosciences
<ul>
<li><strong>SMRT</strong> (single molecule real time) sequencing</li>
<li><img src="23fs/fg/img/20240203114758.png" alt="pacbio1" /></li>
<li><img src="23fs/fg/img/20240203114809.png" alt="pacbio1" /></li>
</ul>
</li>
<li>Oxford <strong>Nanopore</strong>
<ul>
<li>MinION</li>
<li><img src="23fs/fg/img/20240203114829.png" alt="oxford nanopore" /></li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>Self-note</strong>: minimap2 is a popular aligner for long reads.</p>
</blockquote>
<h2 id="environmental-sequencing"><a class="header" href="#environmental-sequencing">Environmental sequencing</a></h2>
<p><u>Traditional genome sequencing</u> requires individual <strong>cell isolation</strong> and <strong>cultivation</strong>. This is not possible for the majority of microorganisms. But one advantage is that it's possible to re-assemble the genome from the reads.</p>
<p>Environmental sequencing: directly sequence DNA extracted from the environment without purification and clonal cultivation. Genome assembly is generally not possible.</p>
<blockquote>
<p><strong>Self-note</strong>: data generated from environmental sequencing is typically large in size, but highly fragmented and contaminated. Lots of exciting research in this area.</p>
</blockquote>
<h3 id="how-to-deal-with-environmental-sequencing-data"><a class="header" href="#how-to-deal-with-environmental-sequencing-data">How to deal with environmental sequencing data</a></h3>
<ul>
<li>Novel gene discovery
<ul>
<li>Sequence identity comparison to known genes
<ul>
<li>But &gt; 50% of the environmental genomes are not similar to any known genome</li>
</ul>
</li>
</ul>
</li>
<li>Novel gene families</li>
<li>Gene family clustering (similar samples have similar gene family distribution)</li>
</ul>
<h2 id="single-cell-sequencing"><a class="header" href="#single-cell-sequencing">Single-cell sequencing</a></h2>
<h3 id="why"><a class="header" href="#why">Why?</a></h3>
<ul>
<li><strong>Heterogeneity</strong> in cell populations
<ul>
<li>Tumor cells</li>
<li>Immune cells</li>
<li>Microbial communities</li>
<li>Developmental biology</li>
</ul>
</li>
</ul>
<h3 id="how"><a class="header" href="#how">How?</a></h3>
<p>In short, we first get <strong>single</strong> cells, then amplify the <strong>whole genome</strong> and sequence it.</p>
<p>The challenges lie in the bolded parts.</p>
<h4 id="single-cell-isolation"><a class="header" href="#single-cell-isolation">Single-cell isolation</a></h4>
<p>(In the very first &quot;single cell&quot; genomics paper, the &quot;single&quot; cells were literally picked manually...nowadays we don't do that)</p>
<ol>
<li>Sorting with optical tweezers
<img src="23fs/fg/img/20240304231454.png" alt="Optic tweezers" /></li>
<li>Dilution series</li>
<li>Flow sorting
<img src="23fs/fg/img/20240304231416.png" alt="Flow sorting" /></li>
</ol>
<h4 id="whole-genome-amplification"><a class="header" href="#whole-genome-amplification">Whole genome amplification</a></h4>
<p>Steps summarized:</p>
<ol>
<li>MDA</li>
<li>phi 29 debranching</li>
<li>S1 nuclease digestion</li>
<li>DNA pol I nick translation</li>
<li>Cloning</li>
</ol>
<ul>
<li>Isothermal <strong>Multiple displacement amplification</strong> (MDA)
<ul>
<li><strong>Phi29</strong> DNA polymerase</li>
<li><strong>Random primers</strong></li>
<li><strong>Isothermal</strong> amplification</li>
</ul>
</li>
</ul>
<p><img src="23fs/fg/img/20240304231641.png" alt="MDA" /></p>
<p>After MDA, we obtained a &quot;<strong>hyperbranched chromosome</strong>&quot;. After <em>debranching</em> and cloning, we can sequence and re-assemble the genome.</p>
<p><img src="23fs/fg/img/20240304231833.png" alt="Next steps" /></p>
<p>The debranching is done by incubating phi 29 DNA pol with hyperbranched DNA <strong>without any primer</strong>. The <em>strand-replacement</em> activity of phi 29 DNA pol will remove the hyperbranched structure.</p>
<p>S1 nucleases are used to remove the remaining single-stranded DNA.</p>
<p>Nicks are filled in by DNA pol I (has 5'-&gt;3' exonuclease activity).</p>
<h2 id="genomic-databases"><a class="header" href="#genomic-databases">Genomic databases</a></h2>
<p>This section likely won't be covered in the exam.</p>
<blockquote>
<p><strong>General popular resources</strong>:</p>
<ul>
<li>Raw data: NCBI <strong>sequence read archive (SRA)</strong> (also it's European counterpart, EBI <strong>European Nucleotide Archive (ENA)</strong>, but they are basically the same thing now)
<ul>
<li>seq quality score included</li>
<li>but incomplete: legacy &amp; newer data not available</li>
<li>gigantic in size</li>
</ul>
</li>
<li>Sequencing projects: <a href="https://gold.jgi.doe.gov/">GOLD (Genomes OnLine Database)</a>
<ul>
<li>keep track of &quot;who is sequencing what&quot; and responsible researchers (contacts), funding sources, sequencing centers etc</li>
</ul>
</li>
<li>Genome browsers
<ul>
<li>Display features (genes, transcripts...) on the genomes, show annotations (conflicts, variants also included), homolog search</li>
<li>UCSC genome browser, Ensembl (popular in Europe)</li>
<li>Pros and cons of genome browsers
<ul>
<li>Pros
<ul>
<li>easy to use</li>
<li>regularly updated</li>
<li>automated annotation pipelines =&gt; fast to include new genomes</li>
<li>very powerful export utilities (<code>BioMart</code> in Ensembl)</li>
<li>API for local access</li>
<li>DAS (distributed annotation system) for data exchange</li>
<li>long term project, stable funding, likely not going away</li>
</ul>
</li>
<li>Cons
<ul>
<li>focus on vertebrates, few other genomes</li>
<li>complex db schema</li>
<li>popular, so can be slow</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Special ones:</p>
<ul>
<li>Comparative genomics databases
<ul>
<li>STRING (protein-protein interactions, focused on microbial genomes, maintained by von Mering group at UZH)</li>
<li>specialized on comparing genomes (at nucleotide-level, or gene-level)</li>
<li>to visualize evidence of selection (exons, regulatory sites, ...)</li>
<li>to infer past evolution of genomes (rearrangements, gains, losses, ...)</li>
<li>to establish gene histories (orthology, paralogy, synteny, ...)</li>
<li>often require extensive offline computation before they go online</li>
<li>some of their services also offered by generic genome browsers/sites.</li>
</ul>
</li>
<li>Organism-specific databases
<ul>
<li>Flybase, Wormbase, TAIR, SGD...</li>
<li>community driven, extensive manual input</li>
<li>specific terms, abbreviations, gene names...</li>
</ul>
</li>
<li>Specialized databases
<ul>
<li>IGSR: human population genetics</li>
<li>OMIM: known disease-causing mutations</li>
<li>KEGG: metabolic pathways and enzymes</li>
</ul>
</li>
</ul>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="modern-genomics-ii"><a class="header" href="#modern-genomics-ii">Modern Genomics II</a></h1>
<p>I cannot understand his accent...</p>
<ul>
<li><a href="23fs/fg/02_modern_genomics_ii.html#contents-that-i-think-are-important">Contents that (I think) are important</a>
<ul>
<li><a href="23fs/fg/02_modern_genomics_ii.html#linkage-disequilibrium-ld">Linkage disequilibrium (LD)</a>
<ul>
<li><a href="23fs/fg/02_modern_genomics_ii.html#coefficient-of-linkage-disequilibrium--d-">Coefficient of linkage disequilibrium ( D )</a></li>
</ul>
</li>
<li><a href="23fs/fg/02_modern_genomics_ii.html#relative-measures-of-ld">Relative measures of LD</a></li>
<li><a href="23fs/fg/02_modern_genomics_ii.html#nucleotide-diversity-pi">Nucleotide diversity (\pi)</a></li>
</ul>
</li>
<li><a href="23fs/fg/02_modern_genomics_ii.html#course-contents-according-to-recording">Course contents according to recording</a></li>
</ul>
<h2 id="contents-that-i-think-are-important"><a class="header" href="#contents-that-i-think-are-important">Contents that (I think) are important</a></h2>
<h3 id="linkage-disequilibrium-ld"><a class="header" href="#linkage-disequilibrium-ld">Linkage disequilibrium (LD)</a></h3>
<h4 id="coefficient-of-linkage-disequilibrium--d-"><a class="header" href="#coefficient-of-linkage-disequilibrium--d-">Coefficient of linkage disequilibrium \( D \)</a></h4>
<p>Given two loci \(A\) and \(B\) (can have allele \(A\) or \(a\), \(B\) or \(b\)), the coefficient of linkage disequilibrium \(D\) is defined as</p>
<p>\[
D = \frac{P_{AB} - P_A P_B}{P_A P_B}
\]</p>
<p>Below shows two examples of \(D\) calculation.</p>
<p><img src="23fs/fg/./img/02-ld-example1.png" alt="LD example 1" /></p>
<p>\[
D = \frac{P_{AB} - P_{A} P_{B}}{P_{A} P_{B}} = \frac{5}{12} - \frac{8}{12}\times\frac{6}{12} = \frac{1}{12}
\]</p>
<p>A and B are linked to some extent.</p>
<p><img src="23fs/fg/./img/02-ld-example2.png" alt="LD example 2" /></p>
<p>\[
D = \frac{P_{AB} - P_{A} P_{B}}{P_{A} P_{B}} = \frac{6}{12} - \frac{8}{12}\times\frac{9}{12} = 0
\]</p>
<p>Here A and B are not linked at all.</p>
<p>Basically we are testing whether P(A) and P(B) are <u>independent</u>.</p>
<h3 id="relative-measures-of-ld"><a class="header" href="#relative-measures-of-ld">Relative measures of LD</a></h3>
<ol>
<li>\(r^2 = \frac{D}{P_A P_a P_B P_b}\)</li>
<li>\(D' = \frac{D}{D_{\max}}\) if \(D\) is positive, \(D' = \frac{D}{D_{\min}}\) if \(D\) is negative</li>
</ol>
<h3 id="nucleotide-diversity-pi"><a class="header" href="#nucleotide-diversity-pi">Nucleotide diversity \(\pi\)</a></h3>
<p><span style="color:gray">Average proportion of pairwise differences between the sequences</span></p>
<p>Give a multiple sequence alignment, \(\pi\) is defined as</p>
<p>\[
\pi = \sum_{i &lt; j} \frac{\pi_{ij}}{n_c}
\]</p>
<p>where \(n_c = \frac{n(n-1)}{2}\) is the number of pairwise comparisons, and \(\pi_{ij}\) is the proportion of differences between the \(i\)th and \(j\)th sequences.</p>
<p>Below shows an example of \(\pi\) calculation.</p>
<p><img src="23fs/fg/./img/02-nt-diversity.png" alt="Nucleotide diversity example" /></p>
<p>\[
\pi = \sum_{i &lt; j} \frac{\pi_{ij}}{n_c} = \frac{\pi_{12} + \pi_{13} + \pi_{23}}{3} = 0.2
\]</p>
<p>Basically just calculating the average number of differences in the matrix.</p>
<h2 id="course-contents-according-to-recording"><a class="header" href="#course-contents-according-to-recording">Course contents according to recording</a></h2>
<p>Title: Applications of next-generation sequencing and other genomic techniques</p>
<ol>
<li>Resequencing to identify genetic basis of phenotypic variation
<ol>
<li>Outlier approach</li>
<li>Genome-wide association studies (<strong>GWAS</strong>) and <strong>linkage disequilibrium</strong></li>
</ol>
</li>
<li>Introduction to RNA-seq (transcriptome) focusing on genome duplication</li>
<li>Resequencing to detect signature of selection: <strong>nucleotide diversity</strong></li>
</ol>
<p>Microbial community and metagenomics, ChIP-seq, epigenomics, whole genome assembly as reference</p>
<ul>
<li>Evolutionary functional genomics can...
<ul>
<li>identify ecologically relevant genes</li>
<li>inferring selection &amp; population processes</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transcriptomics-i"><a class="header" href="#transcriptomics-i">Transcriptomics I</a></h1>
<p>Unimportant text is grayed out but worth reading for fluency and context.</p>
<ul>
<li><a href="23fs/fg/03_transcriptomics_i.html#motivation">Motivation</a></li>
<li><a href="23fs/fg/03_transcriptomics_i.html#rna-species">RNA species</a></li>
<li><a href="23fs/fg/03_transcriptomics_i.html#rna-selection-methods">RNA selection methods</a></li>
<li><a href="23fs/fg/03_transcriptomics_i.html#rna-seq-experiment-workflow-overview">RNA-seq experiment workflow overview</a></li>
<li><a href="23fs/fg/03_transcriptomics_i.html#rna-seq-experiment-design">RNA-seq experiment design</a></li>
<li><a href="23fs/fg/03_transcriptomics_i.html#rna-seq-read-mapping-methods">RNA-seq read mapping methods</a>
<ul>
<li><a href="23fs/fg/03_transcriptomics_i.html#considerations">Considerations</a>
<ul>
<li><a href="23fs/fg/03_transcriptomics_i.html#how-to-map-reads-to-a-genome">How to map reads to a genome?</a></li>
<li><a href="23fs/fg/03_transcriptomics_i.html#what-if-we-want-to-map-billions-of-reads">What if we want to map billions of reads?</a></li>
<li><a href="23fs/fg/03_transcriptomics_i.html#how-to-deal-with-snps-and-indels">How to deal with SNPs and indels?</a></li>
<li><a href="23fs/fg/03_transcriptomics_i.html#how-to-deal-with-introns-basically-very-large-insertions">How to deal with introns (basically very large insertions)?</a></li>
</ul>
</li>
<li><a href="23fs/fg/03_transcriptomics_i.html#bowtie">Bowtie</a></li>
<li><a href="23fs/fg/03_transcriptomics_i.html#tophat2">TopHat2</a></li>
<li><a href="23fs/fg/03_transcriptomics_i.html#star">STAR</a></li>
</ul>
</li>
<li><a href="23fs/fg/03_transcriptomics_i.html#quality-control-and-reporting-considerations-after-mapping">Quality control and reporting considerations after mapping</a>
<ul>
<li><a href="23fs/fg/03_transcriptomics_i.html#phread-quality">PHREAD quality</a></li>
<li><a href="23fs/fg/03_transcriptomics_i.html#read-trimming">Read trimming</a></li>
<li><a href="23fs/fg/03_transcriptomics_i.html#multiple-alignments-and-report-considerations">Multiple alignments and report considerations</a></li>
</ul>
</li>
<li><a href="23fs/fg/03_transcriptomics_i.html#expression-quantification">Expression quantification</a>
<ul>
<li><a href="23fs/fg/03_transcriptomics_i.html#how-to-count-reads">How to count reads?</a>
<ul>
<li><a href="23fs/fg/03_transcriptomics_i.html#em-estimation">EM estimation</a></li>
<li><a href="23fs/fg/03_transcriptomics_i.html#alignment-free-rna-seq-quantification">Alignment-free RNA-seq quantification</a></li>
</ul>
</li>
<li><a href="23fs/fg/03_transcriptomics_i.html#normalization">Normalization</a></li>
</ul>
</li>
</ul>
<h2 id="motivation"><a class="header" href="#motivation">Motivation</a></h2>
<p><span style="color:gray"> We are interested in the expression of genes in a
cell. Altough <u>proteins are the actors</u> of genes instead of mRNA
transcripts, <em>mRNAs transcripts are easier to measure</em> because selection
has chosen them to convey the genetic message in a faithful way. Also there are
plenty of interesting <em>non-coding RNAs</em>. </span></p>
<h2 id="rna-species"><a class="header" href="#rna-species">RNA species</a></h2>
<p>80% of the total RNA is rRNA. 14% is tRNA. Only 1-3% is mRNA. All other species
account for the remaining 1-3%.</p>
<p><img src="23fs/fg/img/03-rna-species.png" alt="RNA species" style="zoom:50%;" /><img
src="img/03-rna-prevalence-pie-chart.png" alt="Prevalence of RNA species"
style="zoom:50%;" /></p>
<h2 id="rna-selection-methods"><a class="header" href="#rna-selection-methods">RNA selection methods</a></h2>
<p><img src="23fs/fg/img/03-rna-selection-method.png" alt="RNA selection methods" /></p>
<ul>
<li>A: total RNA
<ul>
<li>Unbiased estimation</li>
<li>Dominate by rRNA, not very informative</li>
</ul>
</li>
<li>B: ribosomal reduction/depletion
<ul>
<li>Use probes that bind to rRNA and pull them out</li>
<li>&lt; 5% rRNA left</li>
</ul>
</li>
<li>C: poly-A selection
<ul>
<li>Use probes that bind to poly-A tails</li>
<li>Selects for mRNA, and also some long non-coding RNAs (because they also have
poly-A tails)</li>
</ul>
</li>
<li>D: cDNA capture
<ul>
<li>Design target probes that bind to a certain set of RNAs</li>
<li>Limited transcriptome coverage</li>
</ul>
</li>
</ul>
<p><span style="color:gray"> 90% of transcriptome research utilize poly-A
selection; 5% of them go for ribosomal depletion because their mRNAs are of low
quality and fragmented. The rest 5% do have high-quality mRNAs but still go for
ribosomal depletion because they want to estimate other RNA species at the same
time.</span></p>
<p>Note: current illumina sequencers cannot sequence miRNAs (20-25nt) and mRNA
fragments (100-300nt) together (<u>because of the large length discrepency</u>)</p>
<h2 id="rna-seq-experiment-workflow-overview"><a class="header" href="#rna-seq-experiment-workflow-overview">RNA-seq experiment workflow overview</a></h2>
<p>A typical RNA-seq experiment workflow is shown below.</p>
<p><img src="23fs/fg/img/03-rna-seq-experiment-workflow.png" alt="Typical RNA-seq experiment" /></p>
<p><span style="color:gray"> We can do either single-read sequencing or paired-end
sequencing (as is shown in the figure above) in the third step. </span></p>
<p><img src="23fs/fg/img/03-paired-end-seq.png" alt="Paired-end sequencing"
style="zoom:60%;" /></p>
<p>It can also be summarized into a flow diagram.</p>
<p><img src="23fs/fg/img/03-ran-seq-workflow.png" alt="RNA-seq workflow diagram"
style="zoom:67%;" /></p>
<p>The following sections will go into more details about each step.</p>
<h2 id="rna-seq-experiment-design"><a class="header" href="#rna-seq-experiment-design">RNA-seq experiment design</a></h2>
<ul>
<li>RNA-seq experiments are implemented as <strong>comparative experiments</strong></li>
<li>RNA-seq is used to measure the <strong>relative abundance (expression)</strong> of each
mRNA (gene) in a <u>sample</u>
<ul>
<li><span style="color:gray"><u>sample</u></span><span style="color:gray">: We
cannot sequence all the RNA fragments. Instead, we randomly select about 20M
fragments as a representation of the total RNA obtained. Note that we can
sample multiple times. </span></li>
</ul>
</li>
<li><span style="color:gray">RNA-seq can link gene expression to</span>
<ul>
<li><span style="color:gray">genotype</span></li>
<li><span style="color:gray">phenotype</span></li>
<li><span style="color:gray">treatment response </span></li>
</ul>
</li>
</ul>
<h2 id="rna-seq-read-mapping-methods"><a class="header" href="#rna-seq-read-mapping-methods">RNA-seq read mapping methods</a></h2>
<h3 id="considerations"><a class="header" href="#considerations">Considerations</a></h3>
<p><span style="color:gray"> Ideally, the read mapping process is as simple as
<strong>sequencing the transcribed RNAs</strong> and then <strong>mapping the
sequences back to the genome</strong> to identify the gene expression. </span></p>
<p><span style="color:gray"> However, in reality, the process is much more
complicated because <u>current sequencers can only perform faithful sequencing
up to a couple of hundreds of bp (usually 200-800bp)</u>, and a typical mRNA is
2-3kb long. Thus, the mRNA needs to be first chopped into fragments and then
sequenced. When we do the mapping, we're mapping the chopped reads back to the
genome instead of the whole spliced sequences. The existence of <u>alternative
splicing (protein isoforms)</u> and <u>sequencing errors</u> makes the mapping
process even more complicated. </span></p>
<p><span style="color:gray"> While discussing the mapping process, those
limitations and problems should be kept in mind. </span></p>
<h4 id="how-to-map-reads-to-a-genome"><a class="header" href="#how-to-map-reads-to-a-genome">How to map reads to a genome?</a></h4>
<ul>
<li>Mapping algorithms must be
<ul>
<li>Fast
<ul>
<li>Thus usually find perfect or near-perfect (tolerating 1bp mismatch) match</li>
</ul>
</li>
<li>Able to handle SNPs, indels and sequencing errors</li>
</ul>
</li>
</ul>
<p>We can always perform sequence alignment (here needs local
alignment/<strong>Smith-Waterman alignment</strong>). Of course the score matrix should be
good to determine the correct alignment. But this is slow because it requires
quadratic efforts.</p>
<h4 id="what-if-we-want-to-map-billions-of-reads"><a class="header" href="#what-if-we-want-to-map-billions-of-reads">What if we want to map billions of reads?</a></h4>
<ul>
<li><strong>Indexing method</strong>
<ul>
<li>Reads are aligned by <strong>index lookup</strong> instead of $O(n^2)$ seq comparison</li>
<li>Usually hashed so the lookup time is much faster</li>
<li>If lookup fails, actual sequence comparison is performed (more details on
this in next chapter)</li>
<li>Major aligners use the <u><strong>Burrows-Wheeler transform (BWT)</strong></u> to index
the reference genome
<ul>
<li><span style="color:gray">Very small. Even for the human genome the index
fits into 3GB RAM. </span></li>
</ul>
</li>
</ul>
</li>
<li>Example aligners
<ul>
<li><a href="23fs/fg/03_transcriptomics_i.html#bowtie"><strong>bowtie</strong>, <strong>bowtie2</strong></a></li>
<li>BWA</li>
<li><a href="23fs/fg/03_transcriptomics_i.html#tophat2"><strong>TopHat2</strong></a></li>
<li><a href="23fs/fg/03_transcriptomics_i.html#star"><strong>STAR</strong></a></li>
<li>SOAP</li>
<li>...</li>
</ul>
</li>
</ul>
<h4 id="how-to-deal-with-snps-and-indels"><a class="header" href="#how-to-deal-with-snps-and-indels">How to deal with SNPs and indels?</a></h4>
<ul>
<li><strong>SNPs</strong>
<ul>
<li><span style="color:gray">Can always be handled by performing real
Smith-Waterman alignment but expensive </span></li>
<li>At mismatched positions, try all possible bases and search the BWT index
again
<ul>
<li>Computing effort grows exponentially</li>
</ul>
</li>
<li><strong>Gapped alignment</strong>
<ul>
<li>Chop reads into short segments (seeds)</li>
<li>align those seeds in a <u>mismatch-free</u> manner (typically using BWT
index lookup again)</li>
<li>stitch them back together</li>
<li>Might require multiple rounds of chopping</li>
</ul>
</li>
</ul>
</li>
<li><strong>Indels</strong>
<ul>
<li>Can only be found with <strong>gapped alignment</strong> (described above), BWT lookup is
not enough</li>
</ul>
</li>
</ul>
<h4 id="how-to-deal-with-introns-basically-very-large-insertions"><a class="header" href="#how-to-deal-with-introns-basically-very-large-insertions">How to deal with introns (basically very large insertions)?</a></h4>
<ul>
<li>Approaches
<ul>
<li>Map directly to <strong>trapscript sequences</strong> (no intron) not to the genome
<ul>
<li>Pro: introns are not a problem</li>
<li>Cons: unknown genes/isoforms cannot be detected</li>
</ul>
</li>
<li><strong>Spliced alignment</strong> to genome
<ul>
<li>Pro: finds reads from unknown gene loci or unknown isoforms</li>
<li>Con: larger search space, potentially more false positives, wrong
alignment to <u>pseudogenes</u></li>
</ul>
</li>
<li>Combination of above</li>
</ul>
</li>
<li>Difference between spliced alignment and gapped alignment
<ul>
<li>Gapped alignment: longer gaps means high score penalty and lower score</li>
<li>Spliced alignment: long gaps at canonical splice-sites <u>are allowed</u>,
but lacking canonical splice-sites or gaps elsewhere (at non-canonical
splice sites) are penalized</li>
</ul>
</li>
<li>Pseudogenes
<ul>
<li>associated <em>retrotransposed</em> spliced sequence of a certain gene, thus
<em>intron-free</em>, and appears &quot;<em>exonic</em>&quot;</li>
<li>Have almost the same sequence as the original gene</li>
<li>Not functional because they do not have access to promotors so won't be
transcribed</li>
<li>Usually contain more mutations because there are no selection pressures to
keep them intact</li>
<li>Exonic reads will map to both the gene and its pseudogene, but likely
preferring gene placement due to lack of mutations</li>
<li>A spliced read could be incorrectly assigned to the pseudogene as it appears
to be exonic, preventing higher scoring spliced alignments from being
pursued</li>
</ul>
</li>
</ul>
<h3 id="bowtie"><a class="header" href="#bowtie">Bowtie</a></h3>
<p><img src="23fs/fg/img/03-bowtie.png" alt="Bowtie" /></p>
<ul>
<li>Step-by-step backward searching the <u><strong>suffix</strong></u> of the query sequence in
the BWT index
<ul>
<li>Exact matching</li>
</ul>
</li>
<li>Searched suffix appears consecutively in BWT</li>
<li>The size of range in BWT shrinks or remains the same</li>
<li><u><strong>almost does not handle mismatches</strong></u>; a single mismatch will lead to
<u>empty BWT</u> range / failed index lookup
<ul>
<li>mismatches can come from
<ul>
<li>Sequencing error (illumina: $1/1000$)</li>
<li>True variation (SNPs, human mutation rate $\approx 1/10^4$</li>
</ul>
</li>
<li>mismatches are not rare events! at least 10% of &gt;100nt reads</li>
</ul>
</li>
<li>Empty BWT range activates <strong>backtracking</strong>
<ul>
<li>All possible bases are tried at the mismatched position</li>
<li>Gapped alignment</li>
</ul>
</li>
</ul>
<h3 id="tophat2"><a class="header" href="#tophat2">TopHat2</a></h3>
<p><strong>Handles mismatches</strong> by gapped alignment. It <u>first</u> performs (optional)
<u>transcriptome alignment</u> (exonic), and then performs <u>genome
alignment</u> for the unmapped reads. The unmapped reads in the second step are
multi-exon spanning reads and undergo <u>spliced alignment</u> by chopping the
reads into even shorter segments and splice site identification etc.</p>
<blockquote>
<p>The figure is too complicated, not recommended to go into details but still
attached here for reference. <img src="23fs/fg/img/03-tophat2.png" alt="TopHat2 pipeline" /></p>
</blockquote>
<h3 id="star"><a class="header" href="#star">STAR</a></h3>
<p><strong>Direct</strong> alignment of non-contiguous sequences to the reference genome.</p>
<ul>
<li>First find Maximal Mappable Prefix (MMP) of the read
<ul>
<li>MMP is the longest prefix of the read that can be mapped to the reference
genome</li>
<li>MMP is found by searching the BWT index</li>
<li>Possible to tolerate a certain degree of mismatches using extend mode</li>
</ul>
</li>
<li>Stop extending MMP if MMP covers the whole read or encounters a mismatch</li>
<li>In the second case, find the MMP of the remaining suffix of the read</li>
<li>Repeat until the whole read is covered
<ul>
<li>Possible to use trim mode to get rid of A-tail or adapter sequence</li>
</ul>
</li>
</ul>
<img src="23fs/fg/img/03-star.png" alt="STAR" style="zoom:67%;" />
<h2 id="quality-control-and-reporting-considerations-after-mapping"><a class="header" href="#quality-control-and-reporting-considerations-after-mapping">Quality control and reporting considerations after mapping</a></h2>
<h3 id="phread-quality"><a class="header" href="#phread-quality">PHREAD quality</a></h3>
<ul>
<li>Each called base is given a quality score $Q$</li>
<li>$Q = -10 \log_{10}(p)$ (PHRED score)
<ul>
<li>$p$ is the estimated probability of the base being called incorrectly</li>
<li>$Q$ is the negative log of the probability
<ul>
<li>Q-score = 10: prob = 0.1</li>
<li>Q-score = 20: prob = 0.01</li>
<li>Q-score = 30: prob = 0.001</li>
</ul>
</li>
<li><strong>Q30</strong> is often used as a cutoff for high quality reads</li>
</ul>
</li>
<li>How to use: downweight the low-quality bases when computing the alignment
score</li>
<li><span style="color:gray">First introduced for capillary sequencers</span></li>
<li><span style="color:gray">PHRED scores are determined by the sequencer that
directly rates how reliable the measured signal is</span></li>
</ul>
<h3 id="read-trimming"><a class="header" href="#read-trimming">Read trimming</a></h3>
<p>Not all parts of the read are useful for alignment. It is common to trim the
reads.</p>
<ul>
<li>Reasons for trimming:
<ul>
<li>Systemetic errors of sequencer
<ul>
<li>Illumina sequencers have high error rate at the first few bases</li>
<li>Basically all sequencers have increasing error rate towards the end of the
read</li>
</ul>
</li>
<li>Adapter trimming
<ul>
<li>If the inserted DNA/RNA fragment is too short, the read will contain part
of the adapter (since the sequencer generally will sequence 100bp at each
end)</li>
<li>Can be challenging if:
<ul>
<li>The inserted sequence is 90-100bp -- the incorporated adapter sequence
will only be a few bp long</li>
<li>The read has many sequencing errors</li>
</ul>
</li>
</ul>
</li>
<li>3'-bias / degradation
<ul>
<li>The 3' end of the read is generally more likely to be degraded than the 5'
end</li>
<li>Depending on RNA degradation and extration protocols reads may not be
equally distributed along the transcripts</li>
</ul>
</li>
</ul>
</li>
<li>Trimming methods
<ul>
<li><strong>Hard trimming</strong>: trim a fixed number of bases from the beginning and/or
end</li>
<li><strong>Quality trimming</strong>: cut the end of the read as soon as the base quality
drops below a threshold</li>
<li><strong>Soft trimming</strong>: trim the read that cannot be aligned to the reference
genome</li>
</ul>
</li>
</ul>
<h3 id="multiple-alignments-and-report-considerations"><a class="header" href="#multiple-alignments-and-report-considerations">Multiple alignments and report considerations</a></h3>
<ul>
<li>A read may have multiple valid alignments with identical or similarly good
alignment scores
<ul>
<li>Aligners may use different reporting strategies:
<ul>
<li><strong>Randomly</strong> select one alignment from the top-scoring alignments</li>
<li>Report <u>all</u> alignments that are <u>within delta</u> of the
top-scoring alignment; clip if more than $N_{max}$ alignments are found</li>
<li>Report only alignments if they are <strong>unique</strong> (no other alignment within
delta of the alignment score)</li>
<li>Do not report anything if more than $N_{max}$ valid alignments are found</li>
<li>…</li>
</ul>
</li>
</ul>
</li>
<li>Whether a read has a <strong>unique alignment</strong> depends on
<ul>
<li>the read sequence and the sequence homology of the organism</li>
<li>the search algorithm of the aligner</li>
</ul>
</li>
<li>Whether a read has a <strong>single reported alignment</strong> depends <em>additionally</em> on
<ul>
<li>the reporting options strategy of the aligner</li>
</ul>
</li>
</ul>
<h2 id="expression-quantification"><a class="header" href="#expression-quantification">Expression quantification</a></h2>
<ul>
<li>Expression quantification on
<ul>
<li><strong>Gene level</strong>
<ul>
<li>Reads belong to a gene locus</li>
</ul>
</li>
<li><strong>Isoform level</strong>
<ul>
<li>Reads belong to an isoform</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="how-to-count-reads"><a class="header" href="#how-to-count-reads">How to count reads?</a></h3>
<ul>
<li><strong>Unique mapped reads</strong>
<ul>
<li>Multi-reads are ignored – potentially biased quantification</li>
</ul>
</li>
<li><strong>All mapped reads</strong>
<ul>
<li>Assignment of multi-reads need abundance estimation of overlapped
genes/isoforms</li>
</ul>
</li>
</ul>
<p><img src="23fs/fg/img/03-read-counting.png" alt="Read counting" /></p>
<h4 id="em-estimation"><a class="header" href="#em-estimation">EM estimation</a></h4>
<ul>
<li>General steps
<ol>
<li>Estimate abundances based on <u>uniquely mapping reads only</u></li>
<li>For each multi-read, divide it between the transcripts to which it maps, proportionally to their abundances estimated in the first step</li>
<li>Recompute abundances based on updated counts for each transcript
r.</li>
<li>Continue with Step 2</li>
</ol>
</li>
<li>Model the sequencing and analysis process with a likelihood function</li>
<li>E-step: Compute expected read counts given current expression levels</li>
<li>M-step: Compute expression values maximizing likelihood given expected read counts</li>
</ul>
<h4 id="alignment-free-rna-seq-quantification"><a class="header" href="#alignment-free-rna-seq-quantification">Alignment-free RNA-seq quantification</a></h4>
<p>Instead of building BWT, we build hashed k-mer index based on known
transcriptome, and search the k-mers in the reads to map and compute abundance.</p>
<p>Easier to understand through side-by-side comparison with the traditional
alignment dependent method:</p>
<p><img src="23fs/fg/img/03-kmer.png" alt="Alignment dependent v.s. alignment free" /></p>
<p>While allocating k-mer to transcript, we try to make each transcript covered
as uniformly as possible.</p>
<ul>
<li>Pros
<ul>
<li>Accurate and fast in quantifying <strong>KNOWN TRANSCRIPTS</strong></li>
<li>Counts can be aggregated to get gene-level quantification</li>
</ul>
</li>
<li>Cons
<ul>
<li>Less well annotated genomes – less accurate results</li>
<li>No alignments – no information about SNPs</li>
<li>Transcript coverage is not uniform</li>
</ul>
</li>
</ul>
<h3 id="normalization"><a class="header" href="#normalization">Normalization</a></h3>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transcriptomics-ii"><a class="header" href="#transcriptomics-ii">Transcriptomics II</a></h1>
<ul>
<li><a href="23fs/fg/04_transcriptomics_ii.html#methods-of-exploratory-data-analysis">Methods of Exploratory Data Analysis</a></li>
<li><a href="23fs/fg/04_transcriptomics_ii.html#clustering">Clustering</a>
<ul>
<li><a href="23fs/fg/04_transcriptomics_ii.html#distance-measures">Distance measures</a></li>
<li><a href="23fs/fg/04_transcriptomics_ii.html#hierarchical-clustering">Hierarchical Clustering</a></li>
<li><a href="23fs/fg/04_transcriptomics_ii.html#k-means-clustering">k-means Clustering</a></li>
<li><a href="23fs/fg/04_transcriptomics_ii.html#comparison-of-hierarchical-clustering-and-k-means-clustering">Comparison of Hierarchical Clustering and k-means Clustering</a></li>
</ul>
</li>
<li><a href="23fs/fg/04_transcriptomics_ii.html#dimensionality-reduction">Dimensionality Reduction</a>
<ul>
<li><a href="23fs/fg/04_transcriptomics_ii.html#principal-component-analysis-pca">Principal Component Analysis (PCA)</a></li>
<li><a href="23fs/fg/04_transcriptomics_ii.html#t-sne">t-SNE</a></li>
<li><a href="23fs/fg/04_transcriptomics_ii.html#umap">UMAP</a></li>
</ul>
</li>
<li><a href="23fs/fg/04_transcriptomics_ii.html#differential-expression-analysis">Differential Expression Analysis</a></li>
</ul>
<!-- ## Why exploratory data analysis?

tbc

## What do we study

- Good comparability: usually compare systems with small perturbations
  - Majority of the genes on the first diagonal
  - Only a few genes up- / downregulated

NEED FIGURE

- Bad comparability
  - TO FINISH

NEED FIGURE

- Meaningless comparison
  - Between different cell types

![Comparing adipocytes with astrocytes](img/04-20230313162850.png) -->
<h2 id="methods-of-exploratory-data-analysis"><a class="header" href="#methods-of-exploratory-data-analysis">Methods of Exploratory Data Analysis</a></h2>
<p>Typical shape of data we have in hand: a data matrix.</p>
<p><img src="23fs/fg/img/20240203134507.png" alt="Data matrix" /></p>
<p>where the <strong>columns</strong> are the <strong>samples</strong> and the <strong>rows</strong> are the <strong>features</strong> (usually gene expressions). (It can be the other way around, but this is the most common case.)</p>
<ul>
<li><strong>Clustering</strong>
<ul>
<li>Hierarchical clustering</li>
<li>k-means clustering</li>
</ul>
</li>
<li><strong>Dimensionality reduction</strong>
<ul>
<li>Matrix Factorization
<ul>
<li><strong>PCA</strong></li>
<li><strong>MDS</strong> (Multidimensional Scaling)</li>
</ul>
</li>
<li>Graph-based methods
<ul>
<li><strong>t-SNE</strong></li>
<li><strong>UMAP</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="clustering-1"><a class="header" href="#clustering-1">Clustering</a></h2>
<ul>
<li>Goals
<ul>
<li>group <strong>similar samples</strong>
<ul>
<li><span style="color:gray">evaluate similarity between expression profiles of the samples</span></li>
<li><span style="color:gray">test if similarities match the experimental design and effect sizes</span></li>
<li><span style="color:gray">test if variations within condition is smaller than between conditions</span></li>
<li><span style="color:gray">outliers detection</span></li>
</ul>
</li>
<li>group <strong>similar genes</strong>
<ul>
<li><strong>guilt by association</strong>: infer functions of unknown genes from known genes with the same expression pattern (co-expression)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="distance-measures"><a class="header" href="#distance-measures">Distance measures</a></h3>
<p>To cluster samples, we need to first define a &quot;distance&quot; measure between samples.</p>
<p>Commonly used distance measures:</p>
<ul>
<li><strong>Euclidean distance</strong> (typically for clustering <u>samples</u>)
<ul>
<li>Euclidean distance of two profiles $\mathbf{x}$ and $\mathbf{y}$ with $p$ genes (i.e. the distance between two $p$-dimensional vectors $\mathbf{x}$ and $\mathbf{y}$)</li>
<li>$d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{p} \left(x_i - y_i\right)^2}$</li>
<li><strong>Expression values MUST BE LOG SCALE</strong></li>
</ul>
</li>
<li><strong>$1 - \text{corr}(\mathbf{x}, \mathbf{y})$</strong> (typically for clustering <u>genes</u>)
<ul>
<li>Correlation coefficient of two profiles $\mathbf{x}$ and $\mathbf{y}$ with $p$ samples</li>
<li>$\text{corr}(\mathbf{x}, \mathbf{y}) = \frac{\sum_{i=1}^{p}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{p}(x_i - \bar{x})^2 \cdot \sum_{i-1}^{p}(y_i - \bar{y})^2}}$</li>
<li>$\bar{x} = \frac{1}{p}\sum_{i=1}^{p}x_i$</li>
<li>$\bar{y} = \frac{1}{p}\sum_{i=1}^{p}y_i$</li>
</ul>
</li>
</ul>
<h3 id="hierarchical-clustering-1"><a class="header" href="#hierarchical-clustering-1">Hierarchical Clustering</a></h3>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Compute the distance matrix ($n \times (n - 1) / 2 \rightarrow O(n^2)$) between all samples</li>
<li>Find pair with minimal distance and merge them</li>
<li>Update the distance matrix</li>
<li>Repeat 2-3 until all samples are merged</li>
</ol>
<p><strong>Parameters</strong>:</p>
<ul>
<li>distance measure for samples
<ul>
<li>usually $1 - \text{corr}(\mathbf{x}, \mathbf{y})$ for gene expression</li>
</ul>
</li>
<li>distance measure for clusters (<strong>linkage rule</strong>)
<ul>
<li><strong>Single</strong> linkage: <strong>minimum</strong> distance between any elements of the two clusters</li>
<li><strong>Complete</strong> linkage: <strong>maximum</strong> distance between any elements of the two clusters</li>
<li><strong>Average</strong> linkage: <strong>average</strong> distance between <em>all</em> elements of the two clusters</li>
<li><strong>Ward's</strong> linkage: <strong>minimal</strong> increase in <strong>intra-cluster variance</strong></li>
</ul>
</li>
</ul>
<p><strong>Input</strong>:</p>
<ul>
<li><strong>distance matrix</strong>
<ul>
<li>The linkage can be derived directly from the distance matrix.</li>
<li>Hence clustering algorithm only needs the distrance matrix as input, not the measurements individually.</li>
</ul>
</li>
</ul>
<h3 id="k-means-clustering-1"><a class="header" href="#k-means-clustering-1">k-means Clustering</a></h3>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>Randomly assign each sample to one of the $k$ clusters</li>
<li>Compute the centroid (cluster center, average of the assigned samples) of each cluster</li>
<li>Assign each sample to the cluster with the closest centroid</li>
<li>Repeat 2-3 until convergence or a maximum number of iterations</li>
</ol>
<p><strong>Parameters</strong>:</p>
<ul>
<li>number of clusters $k$</li>
<li>distance measure for samples</li>
</ul>
<p><strong>Input</strong>:</p>
<ul>
<li>data matrix (cannot directly use distance matrix)</li>
</ul>
<p>This method <strong>minimizes the intra-cluster variance</strong>.</p>
<p>Choice of $k$ affects the result.</p>
<h3 id="comparison-of-hierarchical-clustering-and-k-means-clustering"><a class="header" href="#comparison-of-hierarchical-clustering-and-k-means-clustering">Comparison of Hierarchical Clustering and k-means Clustering</a></h3>
<div class="table-wrapper"><table><thead><tr><th></th><th>Hierarchical Clustering</th><th>k-means Clustering</th></tr></thead><tbody>
<tr><td>Computing time</td><td>$O(n^2 \log (n))$</td><td>$O(n \cdot k \cdot t)$ <ul><li>$t$: number of iterations</li><li>$k$: number of clusters</li></ul></td></tr>
<tr><td>Memory</td><td>$O(n^2)$</td><td>$O(n \cdot k)$</td></tr>
</tbody></table>
</div>
<p>When clustering large numbers of genes (&gt;<code>1e4</code>, hierarchical clustering is not practical</p>
<h2 id="dimensionality-reduction"><a class="header" href="#dimensionality-reduction">Dimensionality Reduction</a></h2>
<h3 id="principal-component-analysis-pca"><a class="header" href="#principal-component-analysis-pca">Principal Component Analysis (PCA)</a></h3>
<p><strong>Goal</strong>: Find a new coordinate system such that the first axis (principal component) captures the most variance, the second axis captures the second most variance, and so on.</p>
<p>The data is <strong>linearly</strong> transformed to a new coordinate system.</p>
<h3 id="t-sne"><a class="header" href="#t-sne">t-SNE</a></h3>
<p><strong>Algorithm</strong>:</p>
<ol>
<li>In the high-dimensional space, create a <em>probability distribution</em> that dictates how likely two points are to be neighbors.</li>
<li>Recreate a low dimensional space that follows the same probability distribution as best as possible.</li>
</ol>
<p>How to find the best low-dimensional representation:</p>
<ul>
<li><strong>preserve the pairwise distances</strong> between neighboring points in the high-dimensional space</li>
<li>non-linear, different transformations on <u>different regions</u></li>
</ul>
<p>Characteristic:</p>
<ul>
<li>Powerful, but need to fiddle with random seed and perplexity</li>
<li>Non-deterministic</li>
</ul>
<h3 id="umap"><a class="header" href="#umap">UMAP</a></h3>
<p>Uniform Manifold Approximation and Projection</p>
<p>Approach: Find for each point the neighbors and build simplices (simplex: a generalization of the concept of a triangle or tetrahedron to arbitrary dimensions) and then optimize the low-dimensional representation to preserve the simplices.</p>
<h2 id="differential-expression-analysis"><a class="header" href="#differential-expression-analysis">Differential Expression Analysis</a></h2>
<div style="break-before: page; page-break-before: always;"></div><h1 id="evolutionary-dynamics"><a class="header" href="#evolutionary-dynamics">Evolutionary Dynamics</a></h1>
<ul>
<li><a href="24hs/evodynamo/01_what_is_evolution.html">01. What is Evolution</a></li>
<li><a href="24hs/evodynamo/02_quasispecies.html">02. Quasispecies</a></li>
<li><a href="24hs/evodynamo/03_stochastic_modles_of_finite_populations.html">03. Stochastic Modles of Finite Populations</a></li>
<li><a href="24hs/evodynamo/04_evolutionary_dynamics_of_cancer.html">04. Evolutionary Dynamics of Cancer</a></li>
</ul>
<!-- toc -->
<div style="break-before: page; page-break-before: always;"></div><h1 id="what-is-evolution"><a class="header" href="#what-is-evolution">What is Evolution?</a></h1>
<h2 id="key-concepts"><a class="header" href="#key-concepts">Key Concepts</a></h2>
<ul>
<li>
<p><strong>Evolution</strong>:</p>
<ul>
<li>Refers to changes in the frequency of different types within a population over generations.</li>
<li>In biological terms, it specifically describes changes in allele frequencies in a gene pool.</li>
</ul>
</li>
<li>
<p><strong>Reproduction</strong>:</p>
<ul>
<li>Evolution requires a population of reproducing individuals for allele frequencies to evolve.</li>
</ul>
</li>
<li>
<p><strong>Exponential Growth (&quot;Malthusian law&quot;)</strong>:</p>
<ul>
<li><strong>Discrete Model</strong>: Population doubles with each generation. The growth equation is \( x_{t+1} = 2x_t \), leading to \( x_t = x_0 \cdot 2^t \).</li>
<li><strong>Continuous Model</strong>: For continuous time, the growth rate is described by \( x'(t) = r \cdot x(t) \) with solution \( x(t) = x_0 e^{rt} \).</li>
</ul>
</li>
<li>
<p><strong>Cell Death</strong>:</p>
<ul>
<li>Mortality is included in the population model with the equation \( x'(t) = (r - d) \cdot x(t) \), where \(d\) is the death rate.</li>
<li>The <strong>basic reproductive ratio</strong> \( R_0 = \frac{r}{d} \) defines population growth dynamics:
<ul>
<li>\( R_0 &gt; 1 \): Population grows.</li>
<li>\( R_0 &lt; 1 \): Population shrinks to extinction.</li>
<li>\( R_0 = 1 \): Population size remains constant but is unstable.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Logistic Growth</strong>:</p>
<ul>
<li>When a population approaches its carrying capacity \(K\), the growth rate slows according to the logistic equation:
\[
x'(t) = r x(t) \left( 1 - \frac{x(t)}{K} \right)
\]</li>
<li>The population reaches equilibrium at the carrying capacity \(K\).</li>
</ul>
</li>
<li>
<p><strong>Logistic Difference Equation</strong>:</p>
<ul>
<li>In discrete time, the logistic map is described by \( x_{t+1} = a x_t (1 - x_t) \), where \(a\) is the growth rate.</li>
<li>This simple equation can exhibit chaotic behavior for certain values of \(a\).</li>
</ul>
</li>
<li>
<p><strong>Selection</strong>:</p>
<ul>
<li><strong>Independent Types</strong>: For two types \(A\) and \(B\) with exponential growth rates \(a\) and \(b\), their relative proportions evolve according to:
\[
\frac{x(t)}{y(t)} = \frac{x_0}{y_0} e^{(a - b)t}
\]
<ul>
<li>If \(a &gt; b\), type \(A\) outcompetes type \(B\), and vice versa.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Two Competing Types</strong>:</p>
<ul>
<li>When the total population is constrained, the selection dynamics for two competing types \(A\) and \(B\) follow:
\[
x'(t) = x(t)(1 - x(t))(a - b)
\]</li>
<li>The system describes the competition between two types based on their fitness.</li>
</ul>
</li>
<li>
<p><strong>Probability Simplex</strong>:</p>
<ul>
<li>The state of a population with multiple types is represented on a <strong>probability simplex</strong>, where each point corresponds to the relative frequencies of types in a population.</li>
</ul>
</li>
<li>
<p><strong>Subexponential vs. Superexponential Growth</strong>:</p>
<ul>
<li><strong>Subexponential Growth</strong> (\(c &lt; 1\)): Stable coexistence of multiple types, even if one has a fitness advantage.</li>
<li><strong>Superexponential Growth</strong> (\(c &gt; 1\)): One type dominates and drives the other to extinction, leading to an unstable mixed equilibrium.</li>
</ul>
</li>
<li>
<p><strong>Mutation</strong>:</p>
<ul>
<li>Mutation introduces genetic diversity, even in the absence of selection.</li>
<li>For two types with mutation rates \(u_1\) and \(u_2\), the equilibrium frequencies depend on the mutation rates:
\[
x^* = \frac{u_2}{u_1 + u_2}
\]
<ul>
<li>Mutation allows coexistence of types, even with no fitness differences.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Hardy-Weinberg Principle</strong>:</p>
<ul>
<li>Describes how allele frequencies in a large, randomly mating population reach equilibrium after one round of random mating.</li>
<li>Genotype frequencies follow:
\[
x = p^2, \quad y = 2pq, \quad z = q^2
\]</li>
<li>These frequencies remain constant over generations unless external factors such as mutation or selection are introduced.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="key-equations-and-models"><a class="header" href="#key-equations-and-models">Key Equations and Models</a></h2>
<ul>
<li>
<p><strong>Exponential Growth</strong>:
\[
x'(t) = r \cdot x(t) \quad \Rightarrow \quad x(t) = x_0 e^{rt}
\]</p>
<ul>
<li>Describes exponential population growth in continuous time.</li>
</ul>
</li>
<li>
<p><strong>Logistic Growth</strong>:
\[
x'(t) = r \cdot x(t) \left( 1 - \frac{x(t)}{K} \right)
\quad \Rightarrow \quad x(t) = \frac{Kx_0 e^{rt}}{K + x_0 (e^{rt} - 1)}
\]</p>
<ul>
<li>Describes population growth with carrying capacity \(K\).</li>
</ul>
</li>
<li>
<p><strong>Selection Dynamics</strong>:
\[
x'(t) = x(t)(1 - x(t))(a - b)
\]</p>
<ul>
<li>Models competition between two types based on relative fitness.</li>
</ul>
</li>
<li>
<p><strong>Mutation Dynamics</strong>:
\[
x'(t) = u_2 - x(u_1 + u_2) \quad \Rightarrow \quad x^* = \frac{u_2}{u_1 + u_2}
\]</p>
<ul>
<li>Describes coexistence due to mutation, even when both types have equal fitness.</li>
</ul>
</li>
<li>
<p><strong>Hardy-Weinberg Equilibrium</strong>:
\[
p = x + \frac{y}{2}, \quad q = z + \frac{y}{2}
\]</p>
<ul>
<li>Allele frequencies remain constant under random mating.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="summary-points"><a class="header" href="#summary-points">Summary Points</a></h2>
<ul>
<li>Evolution is driven by the reproduction of individuals and changes in allele frequencies over generations.</li>
<li>Population growth can be modeled using exponential and logistic equations, which describe different growth dynamics, including constraints like death and carrying capacity.</li>
<li>Selection dynamics favor individuals with higher relative fitness, leading to the survival of the fittest.</li>
<li>Subexponential growth allows for coexistence, while superexponential growth leads to the dominance of one type.</li>
<li>Mutation introduces genetic variation and can drive coexistence even in the absence of fitness differences.</li>
<li>The Hardy-Weinberg principle explains how allele and genotype frequencies stabilize in a large, randomly mating population.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quasispecies"><a class="header" href="#quasispecies">Quasispecies</a></h1>
<h2 id="key-concepts-1"><a class="header" href="#key-concepts-1">Key Concepts</a></h2>
<ul>
<li>
<p><strong>HIV Infection</strong>:</p>
<ul>
<li>HIV has a short genome (~10,000 bases) and a very high mutation rate (~\(3 \times 10^{-5}\) per base per replication).</li>
<li>Rapid adaptation due to short generation time, large population sizes, and strong selective pressures (e.g., immune response, antiretroviral therapy).</li>
</ul>
</li>
<li>
<p><strong>Sequence Space</strong>:</p>
<ul>
<li><strong>Sequence Space</strong>: Describes the set of all possible genetic sequences of a given length \(L\).</li>
<li>For DNA, the sequence is made from bases \(A, C, G, T\), and for proteins, the sequence is based on 20 amino acids.</li>
<li><strong>Binary Sequences</strong>: Simplified representation using \(0\) and \(1\). For a binary sequence of length \(L\), there are \(2^L\) possible sequences.</li>
<li><strong>Hamming Distance</strong>: Used to measure the difference between two sequences based on the number of mismatched positions.</li>
</ul>
</li>
<li>
<p><strong>Fitness Landscapes</strong>:</p>
<ul>
<li><strong>Definition</strong>: A fitness landscape is a mapping from genotype space to fitness, \(f: G \rightarrow \mathbb{R}\), representing how well an organism can survive and reproduce.</li>
<li><strong>Epistasis</strong>: Interactions between loci (positions on the genome) where the effect of one gene depends on the presence of others, influencing fitness.</li>
</ul>
</li>
<li>
<p><strong>Quasispecies Equation</strong>:</p>
<ul>
<li><strong>Quasispecies</strong>: A population of genetically diverse organisms that evolves under mutation and selection pressure.</li>
<li><strong>Mathematical Representation</strong>:
\[
x'(t) = x(t) \cdot Q \cdot f(t) - x(t) \cdot \phi
\]
<ul>
<li>\(x(t)\): Genotype frequency vector.</li>
<li>\(Q\): Mutation matrix.</li>
<li>\(f\): Fitness landscape vector.</li>
<li>\(\phi\): Average population fitness.</li>
</ul>
</li>
<li>The equilibrium solution of this equation predicts the balance between mutation and selection in a population.</li>
</ul>
</li>
<li>
<p><strong>Properties of the Quasispecies Equation</strong>:</p>
<ul>
<li>If replication is error-free (\(Q = I\)), the equation simplifies to the <strong>selection equation</strong>.</li>
<li>If the mutation matrix \(Q\) is irreducible (strongly connected), there exists a single globally stable equilibrium \(x^*\).</li>
<li>Mutation reduces overall population fitness compared to what would be expected from selection alone.</li>
</ul>
</li>
<li>
<p><strong>Adaptation</strong>:</p>
<ul>
<li>A population adapts by localizing around a peak in the fitness landscape, but mutation can cause individuals to drift away from these peaks.</li>
<li><strong>Error Threshold</strong>: A critical mutation rate above which genetic information cannot be maintained, leading to the &quot;mutational meltdown&quot; phenomenon.</li>
</ul>
</li>
<li>
<p><strong>Error Threshold and HIV</strong>:</p>
<ul>
<li>For HIV, with a genome length \(L = 10^4\) and mutation rate \(u = 3 \times 10^{-5}\), the probability of an error-free genome copy is only 0.74.</li>
<li>With 1 billion new viruses produced each day, a vast number of mutations occur daily.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="key-equations-and-models-1"><a class="header" href="#key-equations-and-models-1">Key Equations and Models</a></h2>
<ul>
<li>
<p><strong>Quasispecies Equation</strong>:
\[
x'(t) = x(t) \cdot Q \cdot f(t) - x(t) \cdot \phi
\]</p>
<ul>
<li>Describes the dynamics of a population under mutation and selection.</li>
</ul>
</li>
<li>
<p><strong>Hamming Distance</strong>:</p>
<ul>
<li>Measures the difference between two sequences by counting mismatched positions.</li>
</ul>
</li>
<li>
<p><strong>Error Threshold</strong>:</p>
<ul>
<li>Describes the mutation rate at which the population cannot maintain genetic information. The condition \(uL &gt; 1\) leads to <strong>mutational meltdown</strong>.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="summary-points-1"><a class="header" href="#summary-points-1">Summary Points</a></h2>
<ul>
<li>HIV's high mutation rate and short generation time lead to extreme evolutionary dynamics, allowing rapid adaptation to selective pressures.</li>
<li>Sequence space and fitness landscapes provide a framework for understanding how populations evolve under mutation and selection.</li>
<li>The quasispecies equation models the balance between mutation and selection in populations, particularly for RNA viruses like HIV.</li>
<li>The concept of an error threshold indicates a mutation rate beyond which genetic information cannot be preserved, leading to population collapse.</li>
<li>Mutation-selection balance is central to quasispecies theory, explaining viral diversity and resistance to treatment.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="stochastic-models-of-finite-populations"><a class="header" href="#stochastic-models-of-finite-populations">Stochastic Models of Finite Populations</a></h1>
<h2 id="key-concepts-2"><a class="header" href="#key-concepts-2">Key Concepts</a></h2>
<ul>
<li>
<p><strong>Gambler’s Ruin</strong>:</p>
<ul>
<li>A classical problem in probability theory where a random walk eventually results in one party (player or bank) losing all resources, illustrating the inevitability of random processes reaching absorbing states.</li>
</ul>
</li>
<li>
<p><strong>Markov Chains</strong>:</p>
<ul>
<li><strong>Definition</strong>: A stochastic process where the next state depends only on the current state, not on the sequence of previous states. Described by a transition matrix.</li>
<li><strong>Ergodicity</strong>: A Markov chain is ergodic if it is aperiodic and irreducible, meaning every state can be reached from any other, and a unique stationary distribution exists.</li>
</ul>
</li>
<li>
<p><strong>The Moran Process</strong>:</p>
<ul>
<li>A <strong>birth-death process</strong> where each step involves selecting one individual to reproduce and one to die in a population of constant size \(N\).</li>
<li><strong>State space</strong>: The number of individuals of type \(A\) in the population, ranging from 0 to \(N\).</li>
<li>The process is a Markov chain with <strong>two absorbing states</strong>: either all individuals are of type \(A\) (fixation) or all are of type \(B\) (extinction).</li>
<li><strong>Neutral Drift</strong>: In the absence of selection, the allele frequencies change randomly due to drift.</li>
</ul>
</li>
<li>
<p><strong>Fixation Probability</strong>:</p>
<ul>
<li>The probability that a mutant allele reaches fixation (becomes the only allele in the population) starting from a certain frequency.</li>
<li>In the Moran process, the fixation probability of a neutral allele is \(x_i = i / N\) where \(i\) is the initial number of individuals with the mutant allele.</li>
</ul>
</li>
<li>
<p><strong>Birth-Death Process</strong>:</p>
<ul>
<li>A process where population changes occur incrementally (by one individual at each step), often modeled using transition probabilities.</li>
<li>Transition probabilities in the Moran process are:
\[
P_{i, i+1} = p(1 - p), \quad P_{i, i-1} = (1 - p)p, \quad P_{i, i} = p^2 + (1 - p)^2
\]</li>
<li>This tri-diagonal matrix structure means the population size can change by at most one individual per step.</li>
</ul>
</li>
<li>
<p><strong>Absorption Probabilities</strong>:</p>
<ul>
<li>The probability of ending in state \(N\) (all type A) when starting from state \(i\) is given by:
\[
x_i = \frac{1 + \sum_{j=1}^{i-1} \prod_{k=1}^{j} \gamma_k}{1 + \sum_{j=1}^{N-1} \prod_{k=1}^{j} \gamma_k}
\]
where \(\gamma_i = \frac{\beta_i}{\alpha_i}\) represents the ratio of death to birth rates for different states.</li>
</ul>
</li>
<li>
<p><strong>Mean Fixation Time</strong>:</p>
<ul>
<li>For large population sizes, the expected time to fixation for a neutral allele is approximately \(N^2[(1 - p) \log(1 - p) + p \log p]\) generations.</li>
<li>The diversity (heterozygosity) decays exponentially at a rate proportional to \(2 / N^2\), quantifying the effect of genetic drift.</li>
</ul>
</li>
<li>
<p><strong>Moran Process with Selection</strong>:</p>
<ul>
<li><strong>Selection</strong> introduces differential reproduction rates. Let \(r_A = r\) be the reproduction rate of type \(A\), and \(r_B = 1\) for type \(B\). The probabilities of reproducing type \(A\) or type \(B\) are modeled as competing exponentials.</li>
<li>Transition probabilities for the Moran process with selection are:
\[
P(T_A &lt; T_B) = \frac{r^i}{r^i + (N - i)}
\]</li>
</ul>
</li>
<li>
<p><strong>Poisson Process</strong>:</p>
<ul>
<li>A counting process where events (e.g., mutations) occur randomly over time, with the number of events in any interval following a Poisson distribution.</li>
<li><strong>Inter-arrival times</strong> between events follow an exponential distribution with parameter \(\lambda\), where \(\lambda t\) is the expected number of events in time \(t\).</li>
</ul>
</li>
<li>
<p><strong>Rate of Evolution</strong>:</p>
<ul>
<li>The rate at which a beneficial mutation takes over the population is proportional to the mutation rate \(u\) and the fixation probability \(\rho\). If the mutant is neutral, the rate of evolution is simply the mutation rate \(R = u\).</li>
</ul>
</li>
<li>
<p><strong>Molecular Clock of Neutral Evolution</strong>:</p>
<ul>
<li>Neutral mutations accumulate at a constant rate, independent of population size, leading to the idea of a molecular clock used to estimate evolutionary timescales.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="key-equations-and-models-2"><a class="header" href="#key-equations-and-models-2">Key Equations and Models</a></h2>
<ul>
<li>
<p><strong>Conditional Probability</strong>:
\[
P(X | Y) = \frac{P(X, Y)}{P(Y)}
\]</p>
<ul>
<li>Bayes' theorem and the law of total probability allow the calculation of posterior probabilities in stochastic models.</li>
</ul>
</li>
<li>
<p><strong>Exponential Distribution</strong>:</p>
<ul>
<li>Probability density function:
\[
f(x) = \lambda e^{-\lambda x}
\]</li>
<li><strong>Memoryless property</strong>: \(P(X &gt; s + t | X &gt; t) = P(X &gt; s)\), meaning the probability of an event occurring in the future does not depend on how long it has already been.</li>
</ul>
</li>
<li>
<p><strong>Moran Process Transition Probabilities</strong>:
\[
P_{i, i+1} = p(1 - p), \quad P_{i, i-1} = (1 - p)p, \quad P_{i, i} = p^2 + (1 - p)^2
\]</p>
<ul>
<li>Describes the random drift of allele frequencies in a finite population.</li>
</ul>
</li>
<li>
<p><strong>Fixation Probability in the Moran Process</strong>:
\[
x_i = \frac{i}{N}
\]</p>
<ul>
<li>For neutral mutations, the probability of fixation is simply the initial frequency of the allele.</li>
</ul>
</li>
<li>
<p><strong>Absorption Probability in Birth-Death Process</strong>:
\[
x_i = \frac{1 + \sum_{j=1}^{i-1} \prod_{k=1}^{j} \gamma_k}{1 + \sum_{j=1}^{N-1} \prod_{k=1}^{j} \gamma_k}
\]</p>
<ul>
<li>Represents the probability of ending up in a certain absorbing state (all type \(A\)).</li>
</ul>
</li>
<li>
<p><strong>Rate of Evolution</strong>:
\[
R = N u \rho
\]</p>
<ul>
<li>Describes how fast a population evolves from one state to another under mutation and selection.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="summary-points-2"><a class="header" href="#summary-points-2">Summary Points</a></h2>
<ul>
<li>The <strong>Moran process</strong> is a fundamental birth-death process in population genetics, capturing the dynamics of allele frequencies in finite populations.</li>
<li><strong>Markov chains</strong> and stochastic processes underpin the mathematical framework, where each state represents a different composition of the population.</li>
<li><strong>Fixation probabilities</strong> and <strong>mean fixation time</strong> can be analytically derived for both neutral and selectively advantageous alleles.</li>
<li><strong>Poisson processes</strong> model random mutation events, and the molecular clock of neutral evolution assumes that mutations accumulate at a constant rate over time.</li>
<li>The <strong>Moran process with selection</strong> introduces competing reproduction rates, influencing the rate at which beneficial alleles spread in the population.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="evolutionary-dynamics-of-cancer"><a class="header" href="#evolutionary-dynamics-of-cancer">Evolutionary Dynamics of Cancer</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="mermaid.min.js"></script>
        <script src="mermaid-init.js"></script>

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
